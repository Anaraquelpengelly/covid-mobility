{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import helper_methods_for_aggregate_data_analysis as helper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from disease_model import * \n",
    "from model_experiments import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker as tick\n",
    "from collections import Counter \n",
    "import datetime\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DATETIME = datetime.datetime(2020, 3, 1, 0)\n",
    "MAX_DATETIME = datetime.datetime(2020, 5, 2, 23)\n",
    "MSAS = ['Atlanta_Sandy_Springs_Roswell_GA',\n",
    "        'Chicago_Naperville_Elgin_IL_IN_WI',\n",
    "        'Dallas_Fort_Worth_Arlington_TX',\n",
    "        'Houston_The_Woodlands_Sugar_Land_TX',\n",
    "        'Los_Angeles_Long_Beach_Anaheim_CA',\n",
    "        'Miami_Fort_Lauderdale_West_Palm_Beach_FL',\n",
    "        'New_York_Newark_Jersey_City_NY_NJ_PA',\n",
    "        'Philadelphia_Camden_Wilmington_PA_NJ_DE_MD',\n",
    "        'San_Francisco_Oakland_Hayward_CA',\n",
    "        'Washington_Arlington_Alexandria_DC_VA_MD_WV']\n",
    "DC_NAME = 'Washington_Arlington_Alexandria_DC_VA_MD_WV'\n",
    "\n",
    "LOWER_PERCENTILE = 2.5\n",
    "UPPER_PERCENTILE = 97.5\n",
    "INCIDENCE_POP = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_to_highlight = 'Washington_Arlington_Alexandria_DC_VA_MD_WV'\n",
    "msa_pretty_name = MSAS_TO_PRETTY_NAMES[msa_to_highlight]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MIN_TIMESTRING_FOR_GRID_SEARCH = '2020_05_23_13_50_41_383327'\n",
    "all_results = []\n",
    "for msa in MSAS:\n",
    "    all_results.append(\n",
    "        evaluate_all_fitted_models_for_msa(\n",
    "            msa, \n",
    "            min_timestring=MIN_TIMESTRING_FOR_GRID_SEARCH))\n",
    "df = pd.concat(all_results)\n",
    "df['MSA_name'] = df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "df['ipf_final_match'] = df['model_init_kwargs'].map(lambda x:x['ipf_final_match'])\n",
    "df = df.loc[df['experiment_to_run'] == 'normal_grid_search']\n",
    "print(\"Filtering for grid search models; total models loaded: %i\" % len(df))\n",
    "ablation_df = df.loc[df['poi_psi'] == 0].copy()\n",
    "non_ablation_df = df.loc[df['poi_psi'] > 0].copy()\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search parameter table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_over_threshold(df, msas, threshold, max_models):\n",
    "\n",
    "    keys = ['home_beta', 'poi_psi', 'p_sick_at_t0']\n",
    "    columns = ['MSA', 'n_models'] + [f'{key}_min' for key in keys] + [f'{key}_max' for key in keys]\n",
    "    model_df = pd.DataFrame(columns=columns)\n",
    "    model_df_idx = 0\n",
    "\n",
    "    for msa_name in msas:\n",
    "        subdf = df[(df['MSA_name'] == msa_name)].copy()\n",
    "\n",
    "        loss_key = 'loss_dict_daily_cases_RMSE'\n",
    "        subdf = subdf.sort_values(by=loss_key)\n",
    "        losses = subdf[loss_key] / subdf.iloc[0][loss_key]\n",
    "        n_models = min(max_models, np.sum(losses <= threshold))\n",
    "\n",
    "        model_dict = {}\n",
    "        model_dict['MSA'] = msa_name\n",
    "        # Best fit model\n",
    "        for key in keys:\n",
    "            model_dict[key] = subdf.iloc[0][key]\n",
    "\n",
    "        for key in keys:\n",
    "            model_dict[f'{key}_max'] = -1000000\n",
    "            model_dict[f'{key}_min'] =  1000000\n",
    "\n",
    "        # Mins and maxes\n",
    "        for subdf_idx in range(n_models):\n",
    "            for key in keys:\n",
    "                min_key = f'{key}_min'\n",
    "                max_key = f'{key}_max'\n",
    "                if model_dict[min_key] > subdf.iloc[subdf_idx][key]:\n",
    "                    model_dict[min_key] = subdf.iloc[subdf_idx][key]\n",
    "                if model_dict[max_key] < subdf.iloc[subdf_idx][key]:\n",
    "                    model_dict[max_key] = subdf.iloc[subdf_idx][key]\n",
    "\n",
    "        model_dict['n_models'] = n_models\n",
    "        model_df = model_df.append(model_dict, ignore_index=True)\n",
    "        print(f'{msa_name:50s}: {n_models:3d}')\n",
    "\n",
    "    return model_df\n",
    "\n",
    "print(f'Loss tolerance: {ACCEPTABLE_LOSS_TOLERANCE}')\n",
    "model_df = get_models_over_threshold(\n",
    "    non_ablation_df, \n",
    "    MSAS,\n",
    "    ACCEPTABLE_LOSS_TOLERANCE, \n",
    "    MAX_MODELS_TO_TAKE_PER_MSA)\n",
    "keys = ['home_beta', 'poi_psi', 'p_sick_at_t0']\n",
    "table_df = pd.DataFrame(columns=['MSA', '# models within RMSE threshold'] + keys)\n",
    "for idx in range(len(model_df)):\n",
    "    model_dict = {}\n",
    "    model_dict['MSA'] = MSAS_TO_PRETTY_NAMES[model_df.iloc[idx]['MSA']]\n",
    "    model_dict['# models within RMSE threshold'] = model_df.iloc[idx]['n_models']\n",
    "    for key in keys:\n",
    "        min_key = f'{key}_min'\n",
    "        max_key = f'{key}_max'\n",
    "        model_dict[key] = f'{model_df.iloc[idx][key]} ({model_df.iloc[idx][min_key]}, {model_df.iloc[idx][max_key]})'\n",
    "    table_df = table_df.append(model_dict, ignore_index=True)\n",
    "table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1, model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = non_ablation_df\n",
    "thing_to_plot = 'cases'\n",
    "prefix_to_save_plot_with = f'trajectory_{thing_to_plot}_oos_vs_full_fit_{msa_pretty_name}'\n",
    "plot_log = False\n",
    "plot_daily_not_cumulative = False\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    \n",
    "    if idx == 0:\n",
    "        title = 'Out-of-sample fit'\n",
    "        plot_legend = False\n",
    "        train_test_partition = TRAIN_TEST_PARTITION\n",
    "        key_to_sort_by = 'train_loss_dict_daily_cases_RMSE'\n",
    "    else:\n",
    "        title = 'Full fit'\n",
    "        plot_legend = True\n",
    "        train_test_partition = None\n",
    "        key_to_sort_by = 'loss_dict_daily_cases_RMSE'\n",
    "        \n",
    "    subdf = df[(df['MSA_name'] == msa_to_highlight)].copy()\n",
    "    subdf = subdf.sort_values(by=key_to_sort_by)    \n",
    "    losses = subdf[key_to_sort_by] / subdf.iloc[0][key_to_sort_by]\n",
    "    num_models_to_aggregate = np.sum(losses <= ACCEPTABLE_LOSS_TOLERANCE)\n",
    "        \n",
    "    mdl_predictions = []\n",
    "    old_projected_hrs = None\n",
    "    for model_idx in range(num_models_to_aggregate):\n",
    "        ts = subdf.iloc[model_idx]['timestring']\n",
    "        _, kwargs, _, model_results, _ = load_model_and_data_from_timestring(ts, load_fast_results_only=False)\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        data_kwargs = kwargs['data_kwargs']\n",
    "        mdl_prediction, projected_hrs = plot_model_fit_from_model_and_kwargs(ax, \n",
    "                                             model_kwargs, \n",
    "                                             data_kwargs, \n",
    "                                             model_results=model_results, \n",
    "                                             plotting_kwargs={'make_plot':False,\n",
    "                                                              'plot_daily_not_cumulative':plot_daily_not_cumulative,                                                              \n",
    "                                                              'return_mdl_pred_and_hours':True}, \n",
    "                                             train_test_partition=train_test_partition,\n",
    "                                         )\n",
    "        mdl_predictions.append(mdl_prediction)\n",
    "        if old_projected_hrs is not None:\n",
    "            assert projected_hrs == old_projected_hrs\n",
    "        old_projected_hrs = projected_hrs\n",
    "    mdl_predictions = np.concatenate(mdl_predictions, axis=0)\n",
    "        \n",
    "    plot_model_fit_from_model_and_kwargs(\n",
    "        ax, \n",
    "        model_kwargs, \n",
    "        data_kwargs,         \n",
    "        plotting_kwargs={\n",
    "            'plot_mode':thing_to_plot, \n",
    "            'plot_log':plot_log, \n",
    "            'plot_legend':plot_legend,\n",
    "            'plot_errorbars':True,\n",
    "            'xticks':[datetime.datetime(2020, 3, 8), \n",
    "                      datetime.datetime(2020, 4, 15),  \n",
    "                      datetime.datetime(2020, 5, 9)],                                                                             \n",
    "            'x_range':[datetime.datetime(2020, 3, 8),\n",
    "                   datetime.datetime(2020, 5, 9)],        \n",
    "            'y_range': (0, 50000),\n",
    "            'plot_daily_not_cumulative':plot_daily_not_cumulative,\n",
    "            'model_line_label': 'Model predictions',\n",
    "            'true_line_label': 'Reported cases',\n",
    "            'title':title,\n",
    "            'title_fontsize':20,\n",
    "            'marker_size':5,\n",
    "            'real_data_color':'tab:orange',\n",
    "            'model_color':'tab:blue',\n",
    "            'only_two_yticks':False,\n",
    "            'mdl_prediction':mdl_predictions,\n",
    "            'projected_hrs':projected_hrs}, \n",
    "        train_test_partition=train_test_partition,\n",
    "        )    \n",
    "\n",
    "    ax.grid(False)\n",
    "    \n",
    "log_scale_string = ', log scale' if plot_log else ''\n",
    "daily_or_cumulative_string = 'daily' if plot_daily_not_cumulative else 'cumulative' \n",
    "title_string = ('Sorting by %s\\nplotting %s %s%s' % (key_to_sort_by, \n",
    "                                                daily_or_cumulative_string, \n",
    "                                                thing_to_plot, \n",
    "                                                 log_scale_string))\n",
    "\n",
    "ax = fig.add_subplot(111, frame_on=False)\n",
    "ax.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "ax.set_ylabel('Cumulative confirmed cases', fontsize=20)\n",
    "ax.yaxis.set_label_coords(-0.11,0.5)\n",
    "\n",
    "if prefix_to_save_plot_with is not None:\n",
    "    for ext in ['svg']:\n",
    "        plt.savefig('covid_figures_for_paper/trajectories/%s.%s' % \n",
    "                (prefix_to_save_plot_with,\n",
    "                ext), \n",
    "                bbox_inches='tight',\n",
    "                dpi=600)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid with top 20% models\n",
    "thing_to_plot = 'cases'\n",
    "plot_log = False\n",
    "plot_daily_not_cumulative = False\n",
    "\n",
    "for df, df_str in [(non_ablation_df, 'non_ablation')]:\n",
    "\n",
    "    for train_test_partition, train_test_partition_str, key_to_sort_by in [\n",
    "        (None, 'full_fit', 'loss_dict_daily_cases_RMSE')]:\n",
    "        \n",
    "        for thing_to_plot in ['cases']:\n",
    "                        \n",
    "            prefix_to_save_plot_with = f'trajectory_3x3_grid_{thing_to_plot}_{train_test_partition_str}_{df_str}_without_{msa_pretty_name}'\n",
    "                    \n",
    "            fig, axes = plt.subplots(3, 3, figsize=(11,9), sharex=True)\n",
    "            axes_list = [ax for axes_row in axes for ax in axes_row]\n",
    "\n",
    "            msas_to_plot_small = [msa for msa in MSAS if msa != msa_to_highlight]\n",
    "            for ax, msa_name in zip(axes_list, msas_to_plot_small):\n",
    "\n",
    "                subdf = df[(df['MSA_name'] == msa_name)].copy()# & (df['home_beta'] == 0.020)]\n",
    "                subdf = subdf.sort_values(by=key_to_sort_by)\n",
    "                losses = subdf[key_to_sort_by] / subdf.iloc[0][key_to_sort_by]\n",
    "                num_models_to_aggregate = np.sum(losses <= ACCEPTABLE_LOSS_TOLERANCE)\n",
    "\n",
    "                mdl_predictions = []\n",
    "                old_projected_hrs = None\n",
    "                for model_idx in range(num_models_to_aggregate):\n",
    "                    ts = subdf.iloc[model_idx]['timestring']\n",
    "                    _, kwargs, _, model_results, _ = load_model_and_data_from_timestring(ts, load_fast_results_only=False)\n",
    "                    model_kwargs = kwargs['model_kwargs']\n",
    "                    data_kwargs = kwargs['data_kwargs']\n",
    "                    mdl_prediction, projected_hrs = plot_model_fit_from_model_and_kwargs(ax, \n",
    "                                                         model_kwargs, \n",
    "                                                         data_kwargs, \n",
    "                                                         model_results=model_results, \n",
    "                                                         plotting_kwargs={\n",
    "                                                             'plot_mode':thing_to_plot, \n",
    "                                                             'make_plot':False,\n",
    "                                                             'plot_daily_not_cumulative':plot_daily_not_cumulative,\n",
    "                                                             'return_mdl_pred_and_hours':True},\n",
    "                                                         train_test_partition=train_test_partition,\n",
    "                                                     )\n",
    "                    mdl_predictions.append(mdl_prediction)\n",
    "                    if old_projected_hrs is not None:\n",
    "                        assert projected_hrs == old_projected_hrs\n",
    "                    old_projected_hrs = projected_hrs\n",
    "                mdl_predictions = np.concatenate(mdl_predictions, axis=0)\n",
    "\n",
    "                \n",
    "                plot_model_fit_from_model_and_kwargs(\n",
    "                    ax, \n",
    "                    model_kwargs, \n",
    "                    data_kwargs,         \n",
    "                    plotting_kwargs={\n",
    "                        'plot_mode':thing_to_plot, \n",
    "                        'plot_log':plot_log, \n",
    "                        'plot_legend':False,\n",
    "                        'plot_errorbars':True,\n",
    "                        'xticks':[datetime.datetime(2020, 3, 8),                                   \n",
    "                                  datetime.datetime(2020, 5, 9)],                                                                             \n",
    "                        'x_range':[datetime.datetime(2020, 3, 8),\n",
    "                               datetime.datetime(2020, 5, 9)],                         \n",
    "                        'plot_daily_not_cumulative':plot_daily_not_cumulative,                        \n",
    "                        'title_fontsize':20,\n",
    "                        'marker_size':5,\n",
    "                        'real_data_color':'tab:orange',\n",
    "                        'model_color':'tab:blue',\n",
    "                        'only_two_yticks':True,\n",
    "                        'mdl_prediction':mdl_predictions,\n",
    "                        'projected_hrs':projected_hrs}, \n",
    "                    train_test_partition=train_test_partition,\n",
    "                    )    \n",
    "\n",
    "\n",
    "                ax.grid(False)\n",
    "            log_scale_string = ', log scale' if plot_log else ''\n",
    "            daily_or_cumulative_string = 'daily' if plot_daily_not_cumulative else 'cumulative' \n",
    "            title_string = ('Sorting by %s\\nplotting %s %s%s' % (key_to_sort_by, \n",
    "                                                            daily_or_cumulative_string, \n",
    "                                                            thing_to_plot, \n",
    "                                                             log_scale_string))\n",
    "            fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "            if prefix_to_save_plot_with is not None:\n",
    "                for ext in ['svg']:\n",
    "                    plt.savefig('covid_figures_for_paper/trajectories/%s.%s' % \n",
    "                            (prefix_to_save_plot_with,\n",
    "                            ext), \n",
    "                            dpi=600)\n",
    "\n",
    "            print(prefix_to_save_plot_with)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplement, cases and deaths for all MSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_log = False\n",
    "plot_daily_not_cumulative = False\n",
    "\n",
    "for df, df_str in [(non_ablation_df, 'non_ablation'), \n",
    "                   (ablation_df, 'ablation')]:\n",
    "\n",
    "    for train_test_partition, train_test_partition_str, key_to_sort_by in [\n",
    "        (None, 'full_fit', 'loss_dict_daily_cases_RMSE'), \n",
    "        (TRAIN_TEST_PARTITION, 'oos_fit', 'train_loss_dict_daily_cases_RMSE')]:\n",
    "        \n",
    "        for thing_to_plot in ['cases', 'deaths']:\n",
    "                        \n",
    "            prefix_to_save_plot_with = f'trajectory_5x2_grid_{thing_to_plot}_{train_test_partition_str}_{df_str}'\n",
    "                    \n",
    "            fig, axes = plt.subplots(5, 2, figsize=(8,15), sharex=True)\n",
    "            axes_list = [ax for axes_row in axes for ax in axes_row]\n",
    "\n",
    "            msas_to_plot_small = [msa for msa in MSAS]\n",
    "            for ax_idx, (ax, msa_name) in enumerate(zip(axes_list, msas_to_plot_small)):\n",
    "\n",
    "                subdf = df[(df['MSA_name'] == msa_name)].copy()\n",
    "                subdf = subdf.sort_values(by=key_to_sort_by)\n",
    "                losses = subdf[key_to_sort_by] / subdf.iloc[0][key_to_sort_by]\n",
    "                num_models_to_aggregate = np.sum(losses <= ACCEPTABLE_LOSS_TOLERANCE)\n",
    "\n",
    "                mdl_predictions = []\n",
    "                old_projected_hrs = None\n",
    "                for model_idx in range(num_models_to_aggregate):\n",
    "                    ts = subdf.iloc[model_idx]['timestring']\n",
    "                    _, kwargs, _, model_results, _ = load_model_and_data_from_timestring(ts, load_fast_results_only=False)\n",
    "                    model_kwargs = kwargs['model_kwargs']\n",
    "                    data_kwargs = kwargs['data_kwargs']\n",
    "                    mdl_prediction, projected_hrs = plot_model_fit_from_model_and_kwargs(ax, \n",
    "                                                         model_kwargs, \n",
    "                                                         data_kwargs, \n",
    "                                                         model_results=model_results, \n",
    "                                                         plotting_kwargs={\n",
    "                                                             'plot_mode':thing_to_plot, \n",
    "                                                             'make_plot':False,\n",
    "                                                             'plot_daily_not_cumulative':plot_daily_not_cumulative,\n",
    "                                                             'return_mdl_pred_and_hours':True},\n",
    "                                                         train_test_partition=train_test_partition,\n",
    "                                                     )\n",
    "                    mdl_predictions.append(mdl_prediction)\n",
    "                    if old_projected_hrs is not None:\n",
    "                        assert projected_hrs == old_projected_hrs\n",
    "                    old_projected_hrs = projected_hrs\n",
    "                mdl_predictions = np.concatenate(mdl_predictions, axis=0)\n",
    "                \n",
    "                if thing_to_plot == 'cases':\n",
    "                    x_start = datetime.datetime(2020, 3, 8)\n",
    "                    real_data_color = 'tab:orange'\n",
    "                    model_color = 'tab:blue'\n",
    "                    model_line_label = 'Model predictions'\n",
    "                    true_line_label = 'Reported cases'                 \n",
    "                elif thing_to_plot == 'deaths':\n",
    "                    x_start = datetime.datetime(2020, 3, 19)\n",
    "                    real_data_color = 'saddlebrown'\n",
    "                    model_color = 'mediumblue'\n",
    "                    model_line_label = 'Model predictions'\n",
    "                    true_line_label = 'Reported deaths'\n",
    "                                    \n",
    "                if train_test_partition_str == 'oos_fit':\n",
    "                    xticks = [x_start, \n",
    "                              datetime.datetime(2020, 4, 15),\n",
    "                              datetime.datetime(2020, 5, 9)]\n",
    "                else:\n",
    "                    xticks = [x_start,                               \n",
    "                              datetime.datetime(2020, 5, 9)]                    \n",
    "                    \n",
    "                plot_model_fit_from_model_and_kwargs(\n",
    "                    ax, \n",
    "                    model_kwargs, \n",
    "                    data_kwargs,         \n",
    "                    plotting_kwargs={\n",
    "                        'plot_mode':thing_to_plot, \n",
    "                        'plot_log':plot_log, \n",
    "                        'plot_legend':False,\n",
    "                        'plot_errorbars':True,\n",
    "                        'xticks':xticks,\n",
    "                        'x_range':[x_start,\n",
    "                                   datetime.datetime(2020, 5, 9)],\n",
    "                        'plot_daily_not_cumulative':plot_daily_not_cumulative,\n",
    "                        'title_fontsize':20,\n",
    "                        'marker_size':5,\n",
    "                        'model_line_label': model_line_label,\n",
    "                        'true_line_label': true_line_label,\n",
    "                        'real_data_color':real_data_color,\n",
    "                        'model_color':model_color,\n",
    "                        'only_two_yticks':True,\n",
    "                        'mdl_prediction':mdl_predictions,\n",
    "                        'projected_hrs':projected_hrs}, \n",
    "                    train_test_partition=train_test_partition,\n",
    "                    )    \n",
    "                if ax_idx == 0:\n",
    "                    ax.legend(bbox_to_anchor=(-0.4, 1.04))\n",
    "\n",
    "                ax.grid(False)                                \n",
    "                \n",
    "            log_scale_string = ', log scale' if plot_log else ''\n",
    "            daily_or_cumulative_string = 'daily' if plot_daily_not_cumulative else 'cumulative' \n",
    "            title_string = ('Sorting by %s\\nplotting %s %s%s' % (key_to_sort_by, \n",
    "                                                            daily_or_cumulative_string, \n",
    "                                                            thing_to_plot, \n",
    "                                                             log_scale_string))\n",
    "            fig.subplots_adjust(wspace=0.3)\n",
    "            \n",
    "\n",
    "            if prefix_to_save_plot_with is not None:\n",
    "                for ext in ['svg']:\n",
    "                    plt.savefig('covid_figures_for_paper/trajectories/%s.%s' % \n",
    "                            (prefix_to_save_plot_with,\n",
    "                            ext), \n",
    "                            bbox_inches='tight',\n",
    "                            dpi=600)\n",
    "\n",
    "            print(prefix_to_save_plot_with)\n",
    "            plt.show()                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobility reductions + reopening analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2a: counterfactual mobility reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2a, left: schematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_distancing_degree(poi_cbg_visits_list, distancing_degree):\n",
    "    new_visits_list = []\n",
    "    for i, m in enumerate(poi_cbg_visits_list):\n",
    "        if i < 168:  # first week\n",
    "            new_visits_list.append(m.copy())\n",
    "        else:\n",
    "            first_week_m = poi_cbg_visits_list[i % 168]\n",
    "            mixture = first_week_m.multiply(1-distancing_degree) + m.multiply(distancing_degree)\n",
    "            new_visits_list.append(mixture.copy())\n",
    "    return new_visits_list\n",
    "\n",
    "def apply_shift_in_days(poi_cbg_visits_list, shift_in_days):\n",
    "    new_visits_list = []\n",
    "    shift_in_hours = shift_in_days * 24\n",
    "    if shift_in_hours <= 0:  # shift earlier\n",
    "        new_visits_list = [m.copy() for m in poi_cbg_visits_list[abs(shift_in_hours):]]\n",
    "        current_length = len(new_visits_list)\n",
    "        assert current_length >= 168\n",
    "        last_week = new_visits_list[-168:]\n",
    "        for i in range(current_length, len(poi_cbg_visits_list)):\n",
    "            last_week_counterpart = last_week[i % 168].copy()\n",
    "            new_visits_list.append(last_week_counterpart)\n",
    "    else:  # shift later\n",
    "        for i in range(len(poi_cbg_visits_list)):\n",
    "            if i-shift_in_hours < 0:\n",
    "                distance_from_start = (shift_in_hours - i) % 168\n",
    "                first_week_idx = 168 - distance_from_start\n",
    "                new_visits_list.append(poi_cbg_visits_list[first_week_idx].copy())\n",
    "            else:\n",
    "                new_visits_list.append(poi_cbg_visits_list[i-shift_in_hours].copy())\n",
    "    return new_visits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_ts(poi_cbg_visits_list):\n",
    "    daily_ts = []\n",
    "    for i in np.arange(0, len(poi_cbg_visits_list), 24):\n",
    "        day_total = 0\n",
    "        for j in range(i, i+24):\n",
    "            day_total += poi_cbg_visits_list[j].sum()\n",
    "        daily_ts.append(day_total)\n",
    "    return daily_ts\n",
    "\n",
    "# https://dfrieds.com/data-visualizations/how-format-large-tick-values.html\n",
    "def reformat_large_tick_values(tick_val, pos):\n",
    "    \"\"\"\n",
    "    Turns large tick values (in the billions, millions and thousands) such as 4500 into 4.5K and also appropriately turns 4000 into 4K (no zero after the decimal).\n",
    "    \"\"\"\n",
    "    if tick_val >= 1000000000:\n",
    "        val = round(tick_val/1000000000, 1)\n",
    "        new_tick_format = '{:}B'.format(val)\n",
    "    elif tick_val >= 1000000:\n",
    "        val = round(tick_val/1000000, 1)\n",
    "        new_tick_format = '{:}M'.format(val)\n",
    "    elif tick_val >= 1000:\n",
    "        val = round(tick_val/1000, 1)\n",
    "        new_tick_format = '{:}K'.format(val)\n",
    "    elif tick_val < 1000:\n",
    "        new_tick_format = round(tick_val, 1)\n",
    "    else:\n",
    "        new_tick_format = tick_val\n",
    "\n",
    "    # make new_tick_format into a string value\n",
    "    new_tick_format = str(new_tick_format)\n",
    "                \n",
    "    return new_tick_format\n",
    "    \n",
    "def make_schematic(orig_visits, lesser_extent, shifted, colors, ax):\n",
    "    days = helper.list_datetimes_in_range(MIN_DATETIME, MAX_DATETIME)\n",
    "    daily_ts = get_daily_ts(orig_visits)\n",
    "    ax.plot_date(days, daily_ts, linestyle='-', marker='.', color=colors[0], label='actual',\n",
    "                linewidth=3)\n",
    "    daily_ts = get_daily_ts(lesser_extent)\n",
    "    ax.plot_date(days, daily_ts, linestyle='-', marker='.', color=colors[1], label='50% of actual',\n",
    "                linewidth=3, alpha=0.5)\n",
    "    daily_ts = get_daily_ts(shifted)\n",
    "    ax.plot_date(days, daily_ts, linestyle='-', marker='.', color=colors[2], label='7 days later',\n",
    "                linewidth=3, alpha=0.5)\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(mdates.SU, interval=2))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "    ax.yaxis.set_major_formatter(tick.FuncFormatter(reformat_large_tick_values))\n",
    "    ax.legend(fontsize=16)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylabel('Total POI visits per day', fontsize=16)\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.set_xlabel('Date', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesser_extent = apply_distancing_degree(orig, 0.5)\n",
    "shifted = apply_shift_in_days(orig, 7)\n",
    "colors = ['black', degree_colors[2], shift_colors[0]]\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "make_schematic(orig, lesser_extent, shifted, colors, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2a, middle and right: results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "counterfactual_results = []\n",
    "min_timestring = '2020_05_29_15_07_37_648695'\n",
    "max_timestring = '2020_06_16'\n",
    "required_properties = {'experiment_to_run':'test_retrospective_counterfactuals'}\n",
    "for msa in MSAS:\n",
    "    counterfactual_results.append(evaluate_all_fitted_models_for_msa(msa, \n",
    "                                                          min_timestring=min_timestring,\n",
    "                                                          max_timestring=max_timestring,\n",
    "                                                          required_properties=required_properties,\n",
    "                                                          key_to_sort_by=None))\n",
    "counterfactual_df = pd.concat(counterfactual_results)\n",
    "counterfactual_df['MSA_name'] = counterfactual_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "assert (counterfactual_df['poi_psi'] > 0).all()\n",
    "print(\"Total models loaded after filtering for counterfactual experiments: %i\" % len(counterfactual_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['distancing_degree', 'shift_in_days']:\n",
    "    counterfactual_df['counterfactual_%s' % k] = counterfactual_df['counterfactual_retrospective_experiment_kwargs'].map(lambda x:x[k] if k in x else np.nan)\n",
    "counterfactual_df['counterfactual_baseline_model'] = counterfactual_df['counterfactual_retrospective_experiment_kwargs'].map(lambda x:x['model_quality_dict']['model_timestring'])\n",
    "counterfactual_df.groupby('MSA_name').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lir_over_time_for_multiple_models(timestrings, ax, label, color):\n",
    "    all_lir = []\n",
    "    hours = None\n",
    "    for ts in timestrings:\n",
    "        _, kwargs, _, model_results, _ = load_model_and_data_from_timestring(ts, load_fast_results_only=False,\n",
    "                                                                             load_full_model=False)\n",
    "\n",
    "        new_hours = [kwargs['model_kwargs']['min_datetime'] + datetime.timedelta(hours=a)\n",
    "                 for a in range(model_results['history']['all']['latent'].shape[1])]\n",
    "        if hours is None:\n",
    "            hours = new_hours\n",
    "        else:\n",
    "            assert list(hours) == list(new_hours) # make sure hours stays unchanged between timestrings.\n",
    "        lir = (model_results['history']['all']['latent'] +\n",
    "           model_results['history']['all']['infected'] +\n",
    "           model_results['history']['all']['removed']) / model_results['history']['all']['total_pop']\n",
    "        all_lir.append(lir)\n",
    "    all_lir = np.concatenate(all_lir, axis=0)\n",
    "    print('Num params x seeds:', all_lir.shape[0])\n",
    "    mean = INCIDENCE_POP * np.mean(all_lir, axis=0)\n",
    "    lower_bound = INCIDENCE_POP * np.percentile(all_lir, LOWER_PERCENTILE, axis=0)\n",
    "    upper_bound = INCIDENCE_POP * np.percentile(all_lir, UPPER_PERCENTILE, axis=0)\n",
    "    ax.plot_date(hours, mean, linestyle='-', label=label, color=color)\n",
    "    ax.fill_between(hours, lower_bound, upper_bound, alpha=0.5, color=color)\n",
    "    ax.set_xlim([min(hours), max(hours)])\n",
    "\n",
    "def make_counterfactual_line_plots(counterfactual_df, msa, ax, mode,\n",
    "                                   cmap_str='viridis', y_lim=None):\n",
    "    assert mode in {'degree', 'shift-later', 'shift-earlier', 'shift'}\n",
    "\n",
    "    \n",
    "    if mode == 'degree':\n",
    "        colors = list(cm.get_cmap(cmap_str, 5).colors)\n",
    "        colors.reverse()\n",
    "        values = [0, .25, .5, np.nan]  # put highest curve first, so that legend order is correct\n",
    "        param_name = 'counterfactual_distancing_degree'\n",
    "        subtitle = 'What if we reduced social distancing?'\n",
    "    else:\n",
    "        colors = list(cm.get_cmap(cmap_str, 6).colors)\n",
    "        colors.reverse()\n",
    "        colors = colors[1:]\n",
    "        if mode == 'shift-later':\n",
    "            values = [14, 7, 3, np.nan]\n",
    "            param_name = 'counterfactual_shift_in_days'\n",
    "            subtitle = 'What if we had started socially distancing x days later?'\n",
    "        elif mode == 'shift-earlier':\n",
    "            values = [np.nan, -3, -7, -14]\n",
    "            param_name = 'counterfactual_shift_in_days'\n",
    "            subtitle = 'What if we had started socially distancing x days earlier?'\n",
    "            colors = colors[3:]  # so that true curve maintains the same color\n",
    "        else:\n",
    "            values = [7, 3, np.nan, -3, -7]\n",
    "            param_name = 'counterfactual_shift_in_days'\n",
    "            subtitle = 'What if we had started earlier or later?'\n",
    "\n",
    "    msa_df = counterfactual_df[counterfactual_df['MSA_name'] == msa]\n",
    "    color_idx = 0\n",
    "    for i, val in enumerate(values):\n",
    "        if np.isnan(val):\n",
    "            # plot baseline models for comparison\n",
    "            timestrings = msa_df.counterfactual_baseline_model.unique()\n",
    "            label = 'actual' if mode == 'degree' else '0 days (actual)'\n",
    "            plot_lir_over_time_for_multiple_models(timestrings, ax, label, 'black')\n",
    "        else:\n",
    "            # plot the models from this experiment\n",
    "            msa_val_df = msa_df[msa_df[param_name] == val]\n",
    "            timestrings = msa_val_df.timestring.values\n",
    "            if mode == 'degree':\n",
    "                label = 'no reduction' if val == 0 else '%d%% of actual' % (val * 100)\n",
    "            else:  # mode is some kind of shift\n",
    "                postfix = 'earlier' if val < 0 else 'later'\n",
    "                label = '%d days %s' % (abs(val), postfix)  # take abs value in case val is negative\n",
    "            plot_lir_over_time_for_multiple_models(timestrings, ax, label, colors[i])\n",
    "\n",
    "    ax.legend(loc='upper left', fontsize=16)\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.SU, interval=2))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "    ax.yaxis.set_major_formatter(tick.FuncFormatter(reformat_with_k))\n",
    "    ax.set_xlabel('Date', fontsize=18)\n",
    "    if y_lim is not None:\n",
    "        ax.set_ylim(y_lim)\n",
    "    ax.grid(alpha=.5)\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.set_title(subtitle, fontsize=18)\n",
    "    return colors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lir_over_time_for_multiple_models(timestrings, ax, label, color):\n",
    "    all_lir = []\n",
    "    hours = None\n",
    "    for ts in timestrings:\n",
    "        _, kwargs, _, model_results, _ = load_model_and_data_from_timestring(ts, load_fast_results_only=False,\n",
    "                                                                             load_full_model=False)\n",
    "\n",
    "        new_hours = [kwargs['model_kwargs']['min_datetime'] + datetime.timedelta(hours=a)\n",
    "                 for a in range(model_results['history']['all']['latent'].shape[1])]\n",
    "        if hours is None:\n",
    "            hours = new_hours\n",
    "        else:\n",
    "            assert list(hours) == list(new_hours) # make sure hours stays unchanged between timestrings.\n",
    "        lir = (model_results['history']['all']['latent'] +\n",
    "           model_results['history']['all']['infected'] +\n",
    "           model_results['history']['all']['removed']) / model_results['history']['all']['total_pop']\n",
    "        all_lir.append(lir)\n",
    "    all_lir = np.concatenate(all_lir, axis=0)\n",
    "    print('Num params x seeds:', all_lir.shape[0])\n",
    "    mean = np.mean(all_lir, axis=0)\n",
    "    lower_bound = np.percentile(all_lir, LOWER_PERCENTILE, axis=0)\n",
    "    upper_bound = np.percentile(all_lir, UPPER_PERCENTILE, axis=0)\n",
    "    ax.plot_date(hours, mean, linestyle='-', label=label, color=color)\n",
    "    ax.fill_between(hours, lower_bound, upper_bound, alpha=0.5, color=color)\n",
    "    ax.set_xlim([min(hours), max(hours)])\n",
    "\n",
    "def make_counterfactual_line_plots(counterfactual_df, msa, ax, mode,\n",
    "                                   cmap_str='viridis', y_lim=None):\n",
    "    assert mode in {'degree', 'shift-later', 'shift-earlier', 'shift'}\n",
    "\n",
    "    colors = list(cm.get_cmap(cmap_str, 5).colors)\n",
    "    colors.reverse()\n",
    "    if mode == 'degree':\n",
    "        values = [0, .25, .5, np.nan]  # put highest curve first, so that legend order is correct\n",
    "        param_name = 'counterfactual_distancing_degree'\n",
    "        subtitle = 'What if we reduced social distancing?'\n",
    "        colors = colors[-len(values):]\n",
    "    elif mode == 'shift-later':\n",
    "        values = [14, 7, 3, np.nan]\n",
    "        param_name = 'counterfactual_shift_in_days'\n",
    "        subtitle = 'What if we had started socially distancing x days later?'\n",
    "    elif mode == 'shift-earlier':\n",
    "        values = [np.nan, -3, -7, -14]\n",
    "        param_name = 'counterfactual_shift_in_days'\n",
    "        subtitle = 'What if we had started socially distancing x days earlier?'\n",
    "        colors = colors[3:]  # so that true curve maintains the same color\n",
    "    else:\n",
    "        values = [7, 3, np.nan, -3, -7]\n",
    "        param_name = 'counterfactual_shift_in_days'\n",
    "        subtitle = 'What if we had started earlier or later?'\n",
    "\n",
    "    msa_df = counterfactual_df[counterfactual_df['MSA_name'] == msa]\n",
    "    color_idx = 0\n",
    "    for i, val in enumerate(values):\n",
    "        if np.isnan(val):\n",
    "            # plot baseline models for comparison\n",
    "            timestrings = msa_df.counterfactual_baseline_model.unique()\n",
    "            label = 'actual' if mode == 'degree' else '0 days (actual)'\n",
    "            plot_lir_over_time_for_multiple_models(timestrings, ax, label, colors[i])\n",
    "        else:\n",
    "            # plot the models from this experiment\n",
    "            msa_val_df = msa_df[msa_df[param_name] == val]\n",
    "            timestrings = msa_val_df.timestring.values\n",
    "            if mode == 'degree':\n",
    "                label = '%d%%' % (val * 100)\n",
    "            else:  # mode is some kind of shift\n",
    "                postfix = 'earlier' if val < 0 else 'later'\n",
    "                label = '%d days %s' % (abs(val), postfix)  # take abs value in case val is negative\n",
    "            plot_lir_over_time_for_multiple_models(timestrings, ax, label, colors[i])\n",
    "\n",
    "    ax.legend(loc='upper left', fontsize=16)\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.SU, interval=2))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "    ax.set_xlabel('Date', fontsize=18)\n",
    "    ax.set_ylabel('Fraction of population infected', fontsize=18)\n",
    "    if y_lim is not None:\n",
    "        ax.set_ylim(y_lim)\n",
    "    ax.grid(alpha=.5)\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.set_title(subtitle, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 7))\n",
    "y_lim = (0, 27000)\n",
    "degree_colors = make_counterfactual_line_plots(counterfactual_df, DC_NAME, axes[0], 'degree', y_lim=y_lim)\n",
    "axes[0].set_title('')\n",
    "shift_colors = make_counterfactual_line_plots(counterfactual_df, DC_NAME, axes[1], 'shift', cmap_str='plasma', y_lim=y_lim)\n",
    "axes[1].set_title('')\n",
    "axes[1].set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplement: table of counterfactual to real infection outcome ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_LIR_fraction_for_multiple_models(timestrings):\n",
    "    final_frac = []\n",
    "    for ts in timestrings:\n",
    "        _, kwargs, _, model_results, _ = load_model_and_data_from_timestring(ts, \n",
    "                                                                             load_fast_results_only=False, \n",
    "                                                                             load_full_model=False)\n",
    "        frac_in_lir = ((model_results['history']['all']['latent'] + \n",
    "                       model_results['history']['all']['infected'] + \n",
    "                       model_results['history']['all']['removed']) /\n",
    "                       model_results['history']['all']['total_pop'])\n",
    "        final_frac.extend(frac_in_lir[:, -1])\n",
    "    return np.array(final_frac)\n",
    "\n",
    "def get_ratios_to_baselines(msa_df, param_name, param_values, baselines):\n",
    "    cols = []\n",
    "    all_ratios = []\n",
    "    for i, val in enumerate(param_values):\n",
    "        msa_val_df = msa_df[msa_df[param_name] == val]\n",
    "        timestrings = msa_val_df.timestring.values\n",
    "        final_fracs = get_final_LIR_fraction_for_multiple_models(timestrings)\n",
    "        ratios = final_fracs / baselines\n",
    "        cols.append('%s=%s' % (param_name.strip('counterfactual_'), val))\n",
    "        mean = np.mean(ratios)\n",
    "        lower_bound = np.percentile(ratios, LOWER_PERCENTILE)\n",
    "        upper_bound = np.percentile(ratios, UPPER_PERCENTILE)\n",
    "        all_ratios.append((round(mean, 3), (round(lower_bound, 3), round(upper_bound, 3))))\n",
    "    return cols, all_ratios\n",
    "    \n",
    "def get_counterfactual_ratios_at_datetime(counterfactual_df, msa):\n",
    "    msa_df = counterfactual_df[counterfactual_df['MSA_name'] == msa].copy()\n",
    "    msa_df = msa_df.sort_values(by='counterfactual_baseline_model')\n",
    "    baseline_timestrings = sorted(msa_df.counterfactual_baseline_model.unique())\n",
    "    baselines = get_final_LIR_fraction_for_multiple_models(baseline_timestrings)\n",
    "    \n",
    "    param_name = 'counterfactual_distancing_degree'\n",
    "    param_values = [0, .25, .5]    \n",
    "    cols, ratios = get_ratios_to_baselines(msa_df, param_name, param_values, baselines)\n",
    "    param_name = 'counterfactual_shift_in_days'\n",
    "    param_values = [7, 3, -3, -7]\n",
    "    cols2, ratios2 = get_ratios_to_baselines(msa_df, param_name, param_values, baselines)\n",
    "    cols.extend(cols2)\n",
    "    ratios.extend(ratios2)\n",
    "    return cols, ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratios = []\n",
    "cols = None\n",
    "for msa in MSAS:\n",
    "    print(msa)\n",
    "    cols, ratios = get_counterfactual_ratios_at_datetime(counterfactual_df, msa)\n",
    "    ratios.insert(0, MSAS_TO_PRETTY_NAMES[msa])\n",
    "    all_ratios.append(ratios)\n",
    "cols.insert(0, 'MSA_name')\n",
    "ratios_df = pd.DataFrame(all_ratios, columns=cols)\n",
    "ratios_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2b: super-spreader POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superspreader_poi_df = []\n",
    "min_timestring = '2020_05_30_14_28_28_220347'\n",
    "required_properties = {'experiment_to_run':'rerun_best_models_and_save_cases_per_poi'}\n",
    "for msa in msas:\n",
    "    superspreader_poi_df.append(evaluate_all_fitted_models_for_msa(msa, \n",
    "                                                      min_timestring=min_timestring,\n",
    "                                                      required_properties=required_properties,\n",
    "                                                      key_to_sort_by=None))\n",
    "superspreader_poi_df = pd.concat(superspreader_poi_df)\n",
    "superspreader_poi_df['MSA_name'] = superspreader_poi_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "print(\"Total models loaded after filtering for superspreader POI analysis: %i\" % len(superspreader_poi_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_characteristics = pickle.load(open('poi_characteristics.pkl', 'rb'))\n",
    "min_datestring = None\n",
    "max_datestring = None\n",
    "fig_with_all_pois = plt.figure(1, figsize=[10, 20])\n",
    "msa_idx = 0\n",
    "all_proportions_of_total_infections_from_pois = []\n",
    "\n",
    "for msa in msas:\n",
    "    print(msa)\n",
    "    city_df = superspreader_poi_df.loc[superspreader_poi_df['MSA_name'] == msa]\n",
    "    all_poi_counts_for_city = []\n",
    "    city_proportions_of_total_infections_from_pois = []\n",
    "    for i in range(len(city_df)):\n",
    "        timestring = city_df.iloc[i]['timestring']\n",
    "        mdl, kwargs, _, _, _ = load_model_and_data_from_timestring(\n",
    "                timestring,\n",
    "                load_fast_results_only=False, \n",
    "                load_full_model=True)\n",
    "        city_proportions_of_total_infections_from_pois += list((mdl.history['all']['new_cases_from_poi'].sum(axis=1)/\n",
    "                                mdl.history['all']['new_cases'].sum(axis=1)))\n",
    "        if min_datestring is None:\n",
    "            min_datestring = kwargs['model_kwargs']['min_datetime'].strftime('%B %-d')\n",
    "        else:\n",
    "            assert min_datestring == kwargs['model_kwargs']['min_datetime'].strftime('%B %-d')\n",
    "            \n",
    "        if max_datestring is None:\n",
    "            max_datestring = kwargs['model_kwargs']['max_datetime'].strftime('%B %-d')\n",
    "        else:\n",
    "            assert max_datestring == kwargs['model_kwargs']['max_datetime'].strftime('%B %-d')\n",
    "            \n",
    "        all_poi_counts_for_city.append(mdl.history['all']['num_cases_per_poi'])\n",
    "        if len(all_poi_counts_for_city) > 1:\n",
    "            assert all_poi_counts_for_city[-1].shape == all_poi_counts_for_city[-2].shape\n",
    "            \n",
    "    all_proportions_of_total_infections_from_pois.append(pd.DataFrame(\n",
    "        {'msa':msa,'prop_total_infections_from_pois':city_proportions_of_total_infections_from_pois}))\n",
    "    # all_poi_counts_for_city is n_seeds x n_pois\n",
    "    all_poi_counts_for_city = np.concatenate(all_poi_counts_for_city, axis=0)\n",
    "    all_poi_fracs_for_city = all_poi_counts_for_city/all_poi_counts_for_city.sum(axis=1).reshape([len(all_poi_counts_for_city), 1])\n",
    "    assert np.allclose(all_poi_fracs_for_city.sum(axis=1), 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mean_frac_per_poi = np.mean(all_poi_fracs_for_city, axis=0)\n",
    "    poi_characteristics_df = poi_characteristics[msa].copy()\n",
    "    poi_characteristics_df['mean_frac_of_infections_at_poi'] = mean_frac_per_poi\n",
    "    poi_characteristics_df['density*dwell_time_factor'] = poi_characteristics_df['dwell_time_correction_factors'] * poi_characteristics_df['weighted_visits_over_area']\n",
    "    poi_characteristics_df['visits^2*dwell_time_factor/area'] = poi_characteristics_df['dwell_time_correction_factors'] * poi_characteristics_df['squared_visits_over_area']\n",
    "    poi_characteristics_df['weighted_visits'] = poi_characteristics_df['weighted_visits_over_area'] * poi_characteristics_df['poi_areas']\n",
    "    poi_characteristics_df['dwell_time'] = invert_dwell_time_correction_factors(poi_characteristics_df['dwell_time_correction_factors'].values)\n",
    "    cutoff_90 = scoreatpercentile(poi_characteristics_df['mean_frac_of_infections_at_poi'], 90)\n",
    "    cutoff_99 = scoreatpercentile(poi_characteristics_df['mean_frac_of_infections_at_poi'], 99)\n",
    "    poi_characteristics_df['infectiousness_group'] = poi_characteristics_df['mean_frac_of_infections_at_poi'].map(lambda x:'top 10%' if x >= cutoff_90\n",
    "                                                                                                                  else 'bottom 90%')\n",
    "    \n",
    "    print(\"Spearman correlations (across POIs) between POI characteristics and fraction of total infections at POI\")\n",
    "    print(poi_characteristics_df.corr(method='spearman')['mean_frac_of_infections_at_poi'])\n",
    "    print(poi_characteristics_df.groupby('infectiousness_group').median().transpose()[['bottom 90%', 'top 10%']])\n",
    "    \n",
    "    sorted_idxs = np.argsort(mean_frac_per_poi)[::-1]\n",
    "    all_poi_fracs_for_city = all_poi_fracs_for_city[:, sorted_idxs]\n",
    "    cumulative_poi_fracs = np.cumsum(all_poi_fracs_for_city, axis=1)\n",
    "    x = np.linspace(0, 1, cumulative_poi_fracs.shape[1])\n",
    "    alpha = 0.05\n",
    "    lower_CI = np.percentile(cumulative_poi_fracs, 100 * alpha/2, axis=0)\n",
    "    upper_CI = np.percentile(cumulative_poi_fracs, 100 * (1 - alpha/2), axis=0)\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=[6, 6])\n",
    "    for subplot_idx, plot_log in enumerate([False]):\n",
    "        ax = fig.add_subplot(1, 1, subplot_idx + 1)\n",
    "\n",
    "        ax.plot(x, cumulative_poi_fracs.mean(axis=0), color='tab:blue')\n",
    "        ax.fill_between(x, lower_CI, upper_CI, color='tab:blue', alpha=.2)\n",
    "        ax.set_xlim([1e-4, 1])\n",
    "        ax.set_ylim([1e-4, 1])\n",
    "        ax.set_xlabel(\"Percent of POIs\", fontsize=14)\n",
    "        ax.set_ylabel(\"Percent of POI infections\", fontsize=14)\n",
    "        if plot_log:\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "        else:\n",
    "            plt.xticks(np.arange(0.0, 1.01, .1), \n",
    "                          ['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "            plt.yticks(np.arange(0.0, 1.01, .1), \n",
    "                          ['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "        ax.grid(alpha=.3)\n",
    "        ax.set_title('Distribution of infections over POIs (%s-%s)' % (min_datestring, max_datestring), fontsize=14)\n",
    "    fig.savefig('covid_figures_for_paper/superspreader_poi_cdf_%s.pdf' % msa, bbox_inches='tight')\n",
    "    \n",
    "    # also make figure for all POIs. \n",
    "    plt.figure(1) # make sure we're modifying the right figure\n",
    "    ax = fig_with_all_pois.add_subplot(5, 2, msa_idx + 1)\n",
    "    ax.plot(x, cumulative_poi_fracs.mean(axis=0), color='tab:blue')\n",
    "    ax.fill_between(x, lower_CI, upper_CI, color='tab:blue', alpha=.2)\n",
    "    ax.set_xlim([1e-4, 1])\n",
    "    ax.set_ylim([1e-4, 1])\n",
    "    if msa_idx % 2 == 0:\n",
    "        ax.set_ylabel(\"Percent of POI infections\", fontsize=14)\n",
    "        \n",
    "    if msa_idx >= 8:\n",
    "        ax.set_xlabel(\"Percent of POIs\", fontsize=14)\n",
    "    plt.xticks(np.arange(0.0, 1.01, .1), \n",
    "                    ['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "    plt.yticks(np.arange(0.0, 1.01, .1), \n",
    "                  ['0%', '10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "    ax.grid(alpha=.3)\n",
    "    ax.set_title(MSAS_TO_PRETTY_NAMES[msa])\n",
    "    msa_idx += 1\n",
    "    \n",
    "#fig_with_all_pois.suptitle('Distribution of infections over POIs (%s-%s)' % (min_datestring, max_datestring), fontsize=14)\n",
    "fig_with_all_pois.savefig('covid_figures_for_paper/superspreader_poi_cdf_all_msas.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2c: partial reopening strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "max_cap_results = []\n",
    "min_timestring = '2020_05_29_09_48_56_478371'\n",
    "max_timestring = '2020_06_16'\n",
    "required_properties = {'experiment_to_run':'test_max_capacity_clipping'}\n",
    "for msa in MSAS:\n",
    "    max_cap_results.append(evaluate_all_fitted_models_for_msa(msa, \n",
    "                                                          min_timestring=min_timestring,\n",
    "                                                          max_timestring=max_timestring,\n",
    "                                                          required_properties=required_properties,\n",
    "                                                          key_to_sort_by=None))\n",
    "max_cap_df = pd.concat(max_cap_results)\n",
    "max_cap_df['MSA_name'] = max_cap_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "assert (max_cap_df['poi_psi'] > 0).all()\n",
    "print(\"Total models loaded after filtering for max capacity experiments: %i\" % len(max_cap_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'max_capacity_alpha'\n",
    "max_cap_df['counterfactual_%s' % k] = max_cap_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x[k] if k in x else np.nan)\n",
    "max_cap_df['counterfactual_baseline_model'] = max_cap_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x['model_quality_dict']['model_timestring'])\n",
    "max_cap_df['baseline_model_quality'] = max_cap_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x['model_quality_dict']['model_fit_rank_for_msa'])\n",
    "max_cap_df.groupby('MSA_name').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_results = []\n",
    "min_timestring = '2020_05_31_20_38_59_228416'\n",
    "required_properties = {'experiment_to_run':'test_uniform_proportion_of_full_reopening'}\n",
    "for msa in MSAS:\n",
    "    uniform_results.append(evaluate_all_fitted_models_for_msa(msa, \n",
    "                                                      min_timestring=min_timestring,\n",
    "                                                      max_timestring=max_timestring,\n",
    "                                                      required_properties=required_properties,\n",
    "                                                      key_to_sort_by=None))\n",
    "uniform_df = pd.concat(uniform_results)\n",
    "uniform_df['MSA_name'] = uniform_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "assert (uniform_df['poi_psi'] > 0).all()\n",
    "print(\"Total models loaded after filtering for uniform reopening experiments: %i\" % len(uniform_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'full_activity_alpha'\n",
    "uniform_df['counterfactual_%s' % k] = uniform_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x[k] if k in x else np.nan)\n",
    "uniform_df['counterfactual_baseline_model'] = uniform_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x['model_quality_dict']['model_timestring'])\n",
    "uniform_df['baseline_model_quality'] = uniform_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x['model_quality_dict']['model_fit_rank_for_msa'])\n",
    "uniform_df.groupby('MSA_name').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_activity_num_visits(msa, intervention_datetime, extra_weeks_to_simulate, min_datetime, max_datetime):\n",
    "    fn = get_ipf_filename(msa, min_datetime, max_datetime, True, True)\n",
    "    f = open(fn, 'rb')\n",
    "    poi_cbg_visits_list = pickle.load(f)\n",
    "    f.close()\n",
    "    all_hours = helper.list_hours_in_range(min_datetime, max_datetime + datetime.timedelta(hours=168 * extra_weeks_to_simulate))\n",
    "    assert(intervention_datetime in all_hours)\n",
    "    intervention_hour_idx = all_hours.index(intervention_datetime)\n",
    "    full_total = 0\n",
    "    for t in range(intervention_hour_idx, len(all_hours)):\n",
    "        full_activity_matrix = poi_cbg_visits_list[t % 168]\n",
    "        full_total += full_activity_matrix.sum()\n",
    "    return full_total, intervention_hour_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2c, left: impacts of clipping strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_with_k(tick_val, pos):\n",
    "    new_tick_val = int(round(tick_val / 1000, 0))\n",
    "    # make new_tick_format into a string value\n",
    "    new_tick_format = '%dk' % new_tick_val                \n",
    "    return new_tick_format\n",
    "\n",
    "def get_lir_checkpoints_and_prop_visits_lost(timestring, intervention_hour_idx, \n",
    "                                             normalize_lir=True, full_activity_num_visits=None,\n",
    "                                             group='all'):\n",
    "    _, _, _, model_results, fast_to_load_results = load_model_and_data_from_timestring(timestring, \n",
    "                                                                                       load_fast_results_only=False,\n",
    "                                                                                       load_full_model=False)\n",
    "    lir = (model_results['history'][group]['latent'] +\n",
    "           model_results['history'][group]['infected'] +\n",
    "           model_results['history'][group]['removed'])\n",
    "    intervention_lir = lir[:, intervention_hour_idx]\n",
    "    final_lir = lir[:, -1]\n",
    "    if normalize_lir:\n",
    "        intervention_lir = intervention_lir / model_results['history'][group]['total_pop']\n",
    "        final_lir = final_lir / model_results['history'][group]['total_pop']\n",
    "    intervention_cost = fast_to_load_results['intervention_cost']\n",
    "    if 'total_activity_after_max_capacity_capping' in intervention_cost:\n",
    "        assert full_activity_num_visits is not None\n",
    "        num_visits = intervention_cost['total_activity_after_max_capacity_capping']\n",
    "        visits_lost = (full_activity_num_visits - num_visits) / full_activity_num_visits\n",
    "    else:\n",
    "        assert 'overall_cost' in intervention_cost\n",
    "        visits_lost = intervention_cost['overall_cost'] / 100\n",
    "    return intervention_lir, final_lir, visits_lost\n",
    "    \n",
    "def make_pareto_plot(results_df, msa, param_name, intervention_idx,\n",
    "                     ax, color=None, cbg_group='all', annotation_color=None, annotate_points=True,\n",
    "                     full_activity_num_visits=None, line_label=None, \n",
    "                     set_axis_labels=True, add_X=None, add_Y=None, add_label=None,\n",
    "                     plot_intervention_lir_line=False, plot_lir_diff=False, only_best_model=False):    \n",
    "    assert not(plot_intervention_lir_line and plot_lir_diff)\n",
    "    msa_df = results_df[results_df['MSA_name'] == msa]\n",
    "    values = sorted([a for a in msa_df[param_name].unique()])\n",
    "    X = []  # prop visits lost\n",
    "    Y_min = []  # frac infections, min\n",
    "    Y_mean = []  # frac infections, mean\n",
    "    Y_max = []  # frac infections, max\n",
    "    all_intervention_lir = []  # frac infections, at the point of intervention\n",
    "    for i, val in enumerate(values):\n",
    "        msa_val_df = msa_df[msa_df[param_name] == val]\n",
    "        if only_best_model:\n",
    "            msa_val_df = msa_val_df[msa_val_df['baseline_model_quality'] == 1]\n",
    "        timestrings = msa_val_df.timestring.values\n",
    "        visits_lost = None\n",
    "        final_lir = []\n",
    "        for ts in timestrings:\n",
    "            curr_intervention_lir, curr_final_lir, curr_visits_lost = get_lir_checkpoints_and_prop_visits_lost(ts, intervention_idx, \n",
    "                                                                                                               group=cbg_group, full_activity_num_visits=full_activity_num_visits)\n",
    "            all_intervention_lir.extend(list(curr_intervention_lir))\n",
    "            if plot_lir_diff:\n",
    "                final_lir.extend(list(curr_final_lir - curr_intervention_lir))\n",
    "            else:\n",
    "                final_lir.extend(list(curr_final_lir))\n",
    "            if visits_lost is None:\n",
    "                visits_lost = curr_visits_lost\n",
    "            else:\n",
    "                assert visits_lost == curr_visits_lost\n",
    "        X.append(visits_lost)\n",
    "        Y_min.append(INCIDENCE_POP * np.percentile(final_lir, LOWER_PERCENTILE))\n",
    "        Y_mean.append(INCIDENCE_POP * np.mean(final_lir))\n",
    "        Y_max.append(INCIDENCE_POP * np.percentile(final_lir, UPPER_PERCENTILE))\n",
    "        if i == 0:\n",
    "            print('Num params * seeds:', len(final_lir))\n",
    "    if add_X is not None and add_Y is not None:\n",
    "        X.append(add_X)\n",
    "        Y_min.append(add_Y[0])\n",
    "        Y_mean.append(add_Y[1])\n",
    "        Y_max.append(add_Y[2])\n",
    "        values.append(1.0)\n",
    "    \n",
    "    if ax is None:\n",
    "        return X, Y_min, Y_mean, Y_max\n",
    "    \n",
    "    if line_label:\n",
    "        ax.plot(X, Y_mean, marker='o', linewidth=2, color=color, label=line_label)\n",
    "    else:\n",
    "        ax.plot(X, Y_mean, marker='o', linewidth=2, color=color)\n",
    "    ax.fill_between(X, Y_min, Y_max, alpha=0.3, color=color)\n",
    "    if annotate_points:\n",
    "        assert annotation_color is not None\n",
    "        for val, x, y in zip(values, X, Y_mean):\n",
    "            if val < .8:\n",
    "                ax.annotate('%d%%' % (100 * val), xy=(x, y), xytext=(0, 5), textcoords='offset pixels', fontsize=16, color=annotation_color)\n",
    "    if plot_intervention_lir_line:\n",
    "        intervention_lir_mean = np.mean(all_intervention_lir)\n",
    "        ax.plot([min(X), max(X)], [INCIDENCE_POP * intervention_lir_mean, INCIDENCE_POP * intervention_lir_mean], color='red', linestyle='--', label='cumulative infections\\nbefore reopening')\n",
    "    ax.yaxis.set_major_formatter(tick.FuncFormatter(reformat_with_k))\n",
    "    if set_axis_labels:\n",
    "        ax.set_xlabel('Fraction of visits lost in month after reopening\\n(compared to full reopening)', fontsize=16)\n",
    "        if plot_lir_diff:\n",
    "            ax.set_ylabel('New infections (per 100k)\\nin month after reopening', fontsize=16)\n",
    "        else:\n",
    "            ax.set_ylabel('Cumulative infections (per 100k)', fontsize=16)\n",
    "        ax.legend(fontsize=16)\n",
    "    ax.grid(alpha=.3)\n",
    "    ax.tick_params(labelsize=16)\n",
    "    return X, Y_min, Y_mean, Y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_full_activity, intervention_idx = get_full_activity_num_visits(DC_NAME, datetime.datetime(2020, 5, 1, 0),\n",
    "                                                                  4, MIN_DATETIME, MAX_DATETIME)\n",
    "print(dc_full_activity, intervention_idx)\n",
    "X, Y_min, Y_mean, Y_max = make_pareto_plot(uniform_df, DC_NAME, 'counterfactual_full_activity_alpha', \n",
    "                                           intervention_idx, None)\n",
    "assert math.isclose(X[-1], 0, abs_tol=1e-8)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "X, Y_min, Y_mean, Y_max = make_pareto_plot(max_cap_df, DC_NAME, 'counterfactual_max_capacity_alpha', intervention_idx,\n",
    "                 ax, 'tab:blue', annotation_color='black',\n",
    "                 full_activity_num_visits=dc_full_activity,\n",
    "                 set_axis_labels=False, add_X=X[-1], add_Y=(Y_min[-1], Y_mean[-1], Y_max[-1]), plot_intervention_lir_line=True)\n",
    "print('Visits lost:', X)\n",
    "print('Visits kept:', 1 - np.array(X))\n",
    "print('Infections:', Y_mean)\n",
    "x_lim = ax.get_xlim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2c, right: comparison of clipping and uniform reduction strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_decimal_as_percent(tick_val, pos):\n",
    "    percent = round(tick_val * 100, 1)\n",
    "    new_tick_format = '%d%%' % percent\n",
    "    return new_tick_format\n",
    "\n",
    "def plot_pairwise_comparison(max_cap_df, uniform_df, msa_name, full_activity_num_visits, intervention_idx,\n",
    "                             ax, cbg_group='all', mode='ratio', color='slategrey', line_label=None, x_lim=None,\n",
    "                             set_axis_labels=True):\n",
    "    assert mode in {'ratio', 'diff', 'percent_change'}\n",
    "    msa_mc_df = max_cap_df[max_cap_df['MSA_name'] == msa_name]\n",
    "    max_cap_vals = sorted(msa_mc_df.counterfactual_max_capacity_alpha.unique())\n",
    "    msa_u_df = uniform_df[uniform_df['MSA_name'] == msa_name]\n",
    "    uniform_vals = sorted(msa_u_df.counterfactual_full_activity_alpha.unique())\n",
    "    assert(len(max_cap_vals) == (len(uniform_vals)-1))\n",
    "    X = []\n",
    "    Y_mean = []\n",
    "    Y_lower = []\n",
    "    Y_upper = []\n",
    "    for mc_val, u_val in zip(max_cap_vals, uniform_vals[:-1]):\n",
    "        print('Comparing max_cap_alpha=%.2f to full_activity_alpha=%.2f' % (mc_val, u_val))\n",
    "        mc_subdf = msa_mc_df[msa_mc_df['counterfactual_max_capacity_alpha'] == mc_val]\n",
    "        u_subdf = msa_u_df[msa_u_df['counterfactual_full_activity_alpha'] == u_val]\n",
    "        mc_all_lir = []\n",
    "        u_all_lir = []\n",
    "        visits_lost = None\n",
    "        for baseline_model in mc_subdf.counterfactual_baseline_model.values:  # per param\n",
    "            curr_df = mc_subdf[mc_subdf['counterfactual_baseline_model'] == baseline_model]\n",
    "            assert len(curr_df) == 1\n",
    "            int_lir, fin_lir, visits_lost = get_lir_checkpoints_and_prop_visits_lost(curr_df.iloc[0]['timestring'], \n",
    "                                                                                     intervention_idx, \n",
    "                                                                                     group=cbg_group,\n",
    "                                                                                     full_activity_num_visits=full_activity_num_visits)\n",
    "            frac_gained = fin_lir - int_lir \n",
    "            mc_all_lir.extend(list(frac_gained.copy()))  # per seed\n",
    "            \n",
    "            curr_df = u_subdf[u_subdf['counterfactual_baseline_model'] == baseline_model]\n",
    "            assert len(curr_df) == 1\n",
    "            int_lir, fin_lir, visits_lost = get_lir_checkpoints_and_prop_visits_lost(curr_df.iloc[0]['timestring'], \n",
    "                                                                                     intervention_idx,\n",
    "                                                                                     group=cbg_group)\n",
    "            frac_gained = fin_lir - int_lir \n",
    "            u_all_lir.extend(list(frac_gained.copy()))\n",
    "        print('Num params * seeds:', len(mc_all_lir))\n",
    "        if mode == 'ratio':\n",
    "            curr_comparison = np.array(mc_all_lir) / np.array(u_all_lir)\n",
    "        elif mode == 'diff':\n",
    "            curr_comparison = np.array(mc_all_lir) - np.array(u_all_lir)\n",
    "        else:\n",
    "            curr_comparison = (np.array(mc_all_lir) - np.array(u_all_lir)) / np.array(u_all_lir)\n",
    "        X.append(visits_lost)\n",
    "        Y_mean.append(np.mean(curr_comparison))\n",
    "        Y_lower.append(np.percentile(curr_comparison, LOWER_PERCENTILE))\n",
    "        Y_upper.append(np.percentile(curr_comparison, UPPER_PERCENTILE))\n",
    "    \n",
    "    if line_label is None:\n",
    "        ax.plot(X, Y_mean, marker='o', linewidth=2, color=color)\n",
    "    else:\n",
    "        ax.plot(X, Y_mean, marker='o', linewidth=2, color=color, label=line_label)\n",
    "    ax.fill_between(X, Y_lower, Y_upper, alpha=0.2, color=color)\n",
    "    if x_lim is not None:\n",
    "        ax.set_xlim(x_lim)\n",
    "    if mode == 'ratio':\n",
    "        ax.plot([min(X), max(X)], [1, 1], color='grey', linestyle='--')\n",
    "        ylabel = 'Ratio of increase from reopening'\n",
    "    else:\n",
    "        ax.plot([min(X), max(X)], [0, 0], color='grey', linestyle='--')\n",
    "        ylabel = 'Difference in increase from reopening'\n",
    "    if set_axis_labels:\n",
    "        ax.set_xlabel('Fraction of visits lost\\n(compared to full reopening)', fontsize=16)\n",
    "        ax.set_ylabel(ylabel, fontsize=16)\n",
    "    ax.grid(alpha=.3)\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.yaxis.set_major_formatter(tick.FuncFormatter(reformat_decimal_as_percent))\n",
    "    return X, Y_mean, Y_lower, Y_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "X, Y_mean, Y_lower, Y_upper = plot_pairwise_comparison(max_cap_df, uniform_df, DC_NAME, dc_full_activity, \n",
    "                                                      intervention_idx, ax, x_lim=x_lim, mode='percent_change',\n",
    "                                                      set_axis_labels=False)\n",
    "print(X)\n",
    "print(Y_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3f: disparities in partial reopening impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we include Figure 3f here with Figure 2c bc they are both analyzing the impacts of the \n",
    "# clipping reopening strategy and they share code\n",
    "msa_name = DC_NAME\n",
    "full_activity = dc_full_activity\n",
    "fig_real, ax_real = plt.subplots(figsize=(7, 7))\n",
    "X, Y_min, Y_mean, Y_max = make_pareto_plot(uniform_df, msa_name, 'counterfactual_full_activity_alpha', intervention_idx,\n",
    "                 None, cbg_group='all', plot_lir_diff=True)\n",
    "X, Y_min, Y_mean, Y_max = make_pareto_plot(max_cap_df, msa_name, 'counterfactual_max_capacity_alpha', intervention_idx,\n",
    "                 ax_real, 'tab:blue', annotate_points=False,\n",
    "                 full_activity_num_visits=full_activity, line_label='overall', \n",
    "                 add_X=X[-1], add_Y=(Y_min[-1], Y_mean[-1], Y_max[-1]),\n",
    "                 cbg_group='all', set_axis_labels=False, plot_lir_diff=True)\n",
    "print('Overall X:', X)\n",
    "print('Overall Y means:', Y_mean)\n",
    "X, Y_min, Y_mean, Y_max = make_pareto_plot(uniform_df, msa_name, 'counterfactual_full_activity_alpha', intervention_idx,\n",
    "                 None, cbg_group=LOWINCOME, plot_lir_diff=True)\n",
    "X, Y_min, Y_mean, Y_max = make_pareto_plot(max_cap_df, msa_name, 'counterfactual_max_capacity_alpha', intervention_idx,\n",
    "                 ax_real, 'darkorchid', annotation_color='darkorchid',\n",
    "                 full_activity_num_visits=full_activity, line_label='bottom income decile', \n",
    "                 add_X=X[-1], add_Y=(Y_min[-1], Y_mean[-1], Y_max[-1]), \n",
    "                 cbg_group=LOWINCOME, set_axis_labels=True, plot_lir_diff=True)\n",
    "print('Bottom decile Y means:', Y_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2d: Reopening different POI categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "\n",
    "intervention_results = []\n",
    "min_timestring = '2020_05_29_18_53_39_321235'\n",
    "for msa in msas:\n",
    "    intervention_results.append(evaluate_all_fitted_models_for_msa(msa, \n",
    "                                                          min_timestring=min_timestring, \n",
    "                                                          key_to_sort_by=None, \n",
    "                                                          required_properties=\n",
    "                                                                   {'experiment_to_run':'test_interventions'}))\n",
    "intervention_df = pd.concat(intervention_results)\n",
    "intervention_df['MSA_name'] = intervention_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "assert (intervention_df['poi_psi'] > 0).all()\n",
    "print(\"Total models loaded after filtering for intervention experiments: %i\" % len(intervention_df))\n",
    "\n",
    "for k in ['alpha', \n",
    "          'extra_weeks_to_simulate', \n",
    "          'intervention_datetime', \n",
    "          'top_category', \n",
    "          'sub_category']:\n",
    "    intervention_df['counterfactual_%s' % k] = intervention_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x[k])\n",
    "intervention_df['model_fit_rank_for_msa'] = intervention_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x['model_quality_dict']['model_fit_rank_for_msa'])\n",
    "\n",
    "assert len(intervention_df) == 6000\n",
    "print(\"Model params\")\n",
    "print(intervention_df.groupby(['MSA_name', 'poi_psi', 'p_sick_at_t0', 'home_beta']).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of categories and make box plots in paper. \n",
    "\n",
    "most_visited_poi_subcategories = get_list_of_poi_subcategories_with_most_visits(n_poi_categories=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for msa in msas:\n",
    "    make_boxplot_of_poi_reopening_effects(intervention_df, \n",
    "                                          [msa], \n",
    "                                          poi_characteristics, \n",
    "                                          titlestring=MSAS_TO_PRETTY_NAMES[msa], \n",
    "                                          cats_to_plot=most_visited_poi_subcategories, \n",
    "                                          filename='covid_figures_for_paper/reopening_impact_boxplots_%s.pdf' % msa)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "make_boxplot_of_poi_reopening_effects(intervention_df, \n",
    "                                      msas,\n",
    "                                      poi_characteristics, \n",
    "                                      titlestring='All MSAs', \n",
    "                                      cats_to_plot=most_visited_poi_subcategories, \n",
    "                                      filename='covid_figures_for_paper/reopening_impact_boxplots_all_MSAs.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disparate impact plots for POI categories (shown in supplement)\n",
    "\n",
    "plot_reopening_effect_by_poi_category_with_disparate_impact(intervention_df, \n",
    "                                      medians_or_deciles='deciles', \n",
    "                                      cats_to_plot=most_visited_poi_subcategories,\n",
    "                                      filename='covid_figures_for_paper/reopening_by_poi_decile_income.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWINCOME = 'median_household_income_bottom_decile'\n",
    "HIGHINCOME = 'median_household_income_top_decile'\n",
    "WHITE = 'p_white_top_decile'\n",
    "NONWHITE = 'p_white_bottom_decile'\n",
    "PATH_TO_SAVED_ATTRIBUTES = os.path.join('/dfs/scratch1/safegraph_homes/all_aggregate_data/', 'msa_data_for_analysis.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to compute relevant POI attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and save relevant POI attributes\n",
    "msa_data_to_save = {}\n",
    "for msa_name in MSAS:\n",
    "    df = helper.load_dataframe_for_individual_msa(MSA_name=msa_name)\n",
    "    data_kwargs = {'MSA_name':msa_name}\n",
    "    model_kwargs = {'min_datetime':MIN_DATETIME,\n",
    "                         'max_datetime':MAX_DATETIME,\n",
    "                         'exogenous_model_kwargs': {\n",
    "                            'home_beta':1e-2,\n",
    "                            'poi_psi':1000,\n",
    "                            'p_sick_at_t0':1e-4,\n",
    "                            'just_compute_r0':False},\n",
    "                          'poi_attributes_to_clip':{'clip_areas':True,\n",
    "                                          'clip_dwell_times':True,\n",
    "                                          'clip_visits':True},\n",
    "                          'correct_poi_visits':True}\n",
    "    # modify fit_ane_save_one_model temporarily to return these things\n",
    "    out = fit_and_save_one_model(None, model_kwargs, data_kwargs, d=df)\n",
    "    msa_data_to_save[msa_name] = {'poi_categories':out[0],\n",
    "                                  'poi_areas':out[1],\n",
    "                                  'poi_dwell_times':out[2],\n",
    "                                  'poi_dwell_time_correction_factors':out[3],\n",
    "                                  'cbg_idx_groups_to_track':out[4],\n",
    "                                  'cbg_sizes':out[5]}\n",
    "f = open(PATH_TO_SAVED_ATTRIBUTES, 'wb')\n",
    "pickle.dump(msa_data_to_save, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can just load the saved attributes without recomputing\n",
    "f = open(PATH_TO_SAVED_ATTRIBUTES, 'rb')\n",
    "msa_to_small_stuff = pickle.load(f)\n",
    "f.close()\n",
    "print(msa_to_small_stuff.keys())\n",
    "\n",
    "MSA_TO_STUFF = msa_to_small_stuff\n",
    "for msa_name in MSAS:\n",
    "    print(msa_name)\n",
    "    fn = get_ipf_filename(msa_name, MIN_DATETIME, MAX_DATETIME, True, True)\n",
    "    print(fn)\n",
    "    f = open(fn, 'rb')\n",
    "    poi_cbg_visits_list = pickle.load(f)\n",
    "    f.close()\n",
    "    MSA_TO_STUFF[msa_name]['poi_cbg_visits_list'] = poi_cbg_visits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poi_attributes_for_msa(msa_name, poi_time_counts=None, group_to_track=None, verbose=True):\n",
    "    '''\n",
    "    Creates a dataframe where each row represents a POI and contains its original\n",
    "    POI index, static attributes of the POI (top_category, sub_category, area, dwell_time,\n",
    "    dwell_time_correction_factor), and its time-varying attributes (avg_occupancy, avg_density, \n",
    "    avg_transmission_rate) averaged over all hours and, if applicable, with respect to the CBGs \n",
    "    specified in group_to_track. \n",
    "    '''\n",
    "    stuff = MSA_TO_STUFF[msa_name]\n",
    "    poi_areas = stuff['poi_areas']\n",
    "    poi_dwell_times = stuff['poi_dwell_times']\n",
    "    poi_dwell_time_correction_factors = stuff['poi_dwell_time_correction_factors']\n",
    "    poi_categories = stuff['poi_categories']\n",
    "    poi_categories['pretty_sub_category'] = poi_categories['sub_category'].map(lambda x:SUBCATEGORIES_TO_PRETTY_NAMES[x] if x in SUBCATEGORIES_TO_PRETTY_NAMES else x)\n",
    "    if poi_time_counts is None:  # use saved poi_cbg_visits_list\n",
    "        poi_cbg_visits_list = stuff['poi_cbg_visits_list']\n",
    "        T = len(poi_cbg_visits_list)\n",
    "        num_pois, num_cbgs = poi_cbg_visits_list[0].shape\n",
    "    else:\n",
    "        num_pois, T = poi_time_counts.shape\n",
    "        num_cbgs = None\n",
    "        poi_cbg_visits_list = None\n",
    "        assert group_to_track is None  # can't differentiate CBGs if we only have overall POI visit counts\n",
    "    if group_to_track is None:\n",
    "        cbg_idx = None\n",
    "    else:\n",
    "        cbg_idx = stuff['cbg_idx_groups_to_track'][group_to_track]\n",
    "    \n",
    "    print('Aggregating data from %d hours' % T)\n",
    "    static_factors = poi_dwell_time_correction_factors / poi_areas\n",
    "    total_weighted_transmission_rates = np.zeros(num_pois)\n",
    "    total_weighted_occupancy = np.zeros(num_pois)\n",
    "    total_weighted_density = np.zeros(num_pois) \n",
    "    total_poi_visits = np.zeros(num_pois)  # denominator: only visits from CBGs of interest\n",
    "    \n",
    "    for t in range(T):\n",
    "        if poi_time_counts is None:\n",
    "            if cbg_idx is None:\n",
    "                indicator = np.ones(num_cbgs)\n",
    "            else:\n",
    "                indicator = np.zeros(num_cbgs)\n",
    "                indicator[cbg_idx] = 1.0\n",
    "            poi_cbg_visits = poi_cbg_visits_list[t]\n",
    "            poi_visits_from_any_cbg = poi_cbg_visits @ np.ones(num_cbgs)\n",
    "            poi_visits_from_cbgs_of_interest = poi_cbg_visits @ indicator\n",
    "        else:\n",
    "            poi_visits_from_any_cbg = poi_time_counts[:, t]\n",
    "            poi_visits_from_cbgs_of_interest = poi_visits_from_any_cbg\n",
    "        \n",
    "        poi_densities = poi_visits_from_any_cbg / poi_areas \n",
    "        poi_transmission_rates = poi_visits_from_any_cbg * static_factors\n",
    "        total_weighted_transmission_rates = total_weighted_transmission_rates + (poi_visits_from_cbgs_of_interest * poi_transmission_rates)\n",
    "        total_weighted_occupancy = total_weighted_occupancy + (poi_visits_from_cbgs_of_interest * poi_visits_from_any_cbg)\n",
    "        total_weighted_density = total_weighted_density + (poi_visits_from_cbgs_of_interest * poi_densities)\n",
    "        total_poi_visits = total_poi_visits + poi_visits_from_cbgs_of_interest\n",
    "\n",
    "    kept_idx = np.array(range(num_pois))[total_poi_visits >= 1]  # only want pois with at least 1 visits\n",
    "    if verbose:\n",
    "        print('Dropped %d/%d POIs with 0 visits in this time period.' % (num_pois - len(kept_idx), num_pois))\n",
    "    poi_attributes_df = pd.DataFrame.from_dict({'poi_idx':kept_idx, \n",
    "                                         'top_category':poi_categories.top_category.values[kept_idx],\n",
    "                                         'sub_category':poi_categories.sub_category.values[kept_idx],\n",
    "                                         'pretty_sub_category':poi_categories.pretty_sub_category.values[kept_idx],\n",
    "                                         'total_num_visits':total_poi_visits[kept_idx],\n",
    "                                         'area':poi_areas[kept_idx].copy(), \n",
    "                                         'dwell_time':poi_dwell_times[kept_idx].copy(),\n",
    "                                         'dwell_time_correction_factor':poi_dwell_time_correction_factors[kept_idx].copy(),\n",
    "                                         'avg_occupancy':total_weighted_occupancy[kept_idx] / total_poi_visits[kept_idx],\n",
    "                                         'avg_density':total_weighted_density[kept_idx] / total_poi_visits[kept_idx],\n",
    "                                         'avg_transmission_rate':total_weighted_transmission_rates[kept_idx] / total_poi_visits[kept_idx],\n",
    "                                         })\n",
    "    return poi_attributes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_attributes_from_poi_attributes(poi_attributes_df, categories, pop_size=None, verbose=True):\n",
    "    category_num_visits = []\n",
    "    category_avg_areas = []  # weighted average over POIs\n",
    "    category_median_areas = []  # median (no weighting) over POIs\n",
    "    category_avg_dwell_times = []\n",
    "    category_median_dwell_times = []\n",
    "    category_avg_occupancies = []\n",
    "    category_median_occupancies = []\n",
    "    category_avg_densities = []\n",
    "    category_median_densities = []\n",
    "    category_avg_transmission_rates = []\n",
    "    category_median_transmission_rates = []\n",
    "    for cat in categories:\n",
    "        subdf = poi_attributes_df[poi_attributes_df['pretty_sub_category'] == cat]\n",
    "        if len(subdf) > 0:\n",
    "            num_visits_per_poi = subdf['total_num_visits'].values\n",
    "            category_num_visits.append(np.sum(num_visits_per_poi))\n",
    "            prop_visits_per_poi = num_visits_per_poi / np.sum(num_visits_per_poi)\n",
    "            attributes_per_poi = subdf[['area', 'dwell_time_correction_factor', 'avg_occupancy',\n",
    "                                        'avg_density', 'avg_transmission_rate']].values\n",
    "            attribute_averages = (prop_visits_per_poi @ attributes_per_poi).T\n",
    "            attribute_medians = np.median(attributes_per_poi, axis=0)  # median over POIs\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Missing visits to any POI in %s' % cat)\n",
    "            category_num_visits.append(0)\n",
    "            attribute_averages = np.ones(5) * np.nan\n",
    "            attribute_medians = np.ones(5) * np.nan\n",
    "        category_avg_areas.append(attribute_averages[0])\n",
    "        category_median_areas.append(attribute_medians[0])\n",
    "        category_avg_dwell_times.append(attribute_averages[1])\n",
    "        category_median_dwell_times.append(attribute_medians[1])\n",
    "        category_avg_occupancies.append(attribute_averages[2])\n",
    "        category_median_occupancies.append(attribute_medians[2])\n",
    "        category_avg_densities.append(attribute_averages[3])\n",
    "        category_median_densities.append(attribute_medians[3])\n",
    "        category_avg_transmission_rates.append(attribute_averages[4])\n",
    "        category_median_transmission_rates.append(attribute_medians[4])\n",
    "    category_attributes_df = pd.DataFrame.from_dict({'category':categories,\n",
    "                                                     'total_num_visits':category_num_visits,\n",
    "                                                     'avg_area':category_avg_areas,\n",
    "                                                     'median_area':category_median_areas,\n",
    "                                                     'avg_dwell_time_correction_factor':category_avg_dwell_times,\n",
    "                                                     'median_dwell_time_correction_factor':category_median_dwell_times,\n",
    "                                                     'avg_occupancy':category_avg_occupancies,\n",
    "                                                     'median_occupancy':category_median_occupancies,\n",
    "                                                     'avg_density':category_avg_densities,\n",
    "                                                     'median_density':category_median_densities,\n",
    "                                                     'avg_transmission_rate':category_avg_transmission_rates,\n",
    "                                                     'median_transmission_rate':category_median_transmission_rates})\n",
    "    if pop_size is not None:\n",
    "        category_attributes_df['num_visits_per_capita'] = category_attributes_df.total_num_visits.values / pop_size\n",
    "    return category_attributes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_CATEGORIES = get_list_of_poi_subcategories_with_most_visits(n_poi_categories=20)\n",
    "PRETTY_TOP_CATEGORIES = [SUBCATEGORIES_TO_PRETTY_NAMES[cat] for cat in TOP_CATEGORIES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3a/b: disparate impact plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LIR_ratios_from_models(timestrings, g1, g2):\n",
    "    \"\"\"\n",
    "    Returns g1/g2. One row for each model, one column for \n",
    "    \"\"\"\n",
    "    LIR_ratios = []\n",
    "    for timestring in timestrings:\n",
    "        mdl, _, _, _, _ = load_model_and_data_from_timestring(\n",
    "            timestring,\n",
    "            load_fast_results_only=False, \n",
    "            load_full_model=True)\n",
    "\n",
    "        ratios = []\n",
    "        for idx, g in enumerate([g1, g2]):        \n",
    "            n_group = mdl.history[g]['total_pop']\n",
    "            n_LIR = (mdl.history[g]['latent'] + mdl.history[g]['infected'] + mdl.history[g]['removed'])[:, -1]\n",
    "            ratios.append(n_LIR / n_group)\n",
    "            \n",
    "\n",
    "\n",
    "        LIR_ratios.append(ratios[0] / ratios[1])    \n",
    "    LIR_ratios = np.array(LIR_ratios)\n",
    "    assert len(LIR_ratios) == len(timestrings)\n",
    "    return LIR_ratios\n",
    "\n",
    "SES_results = []\n",
    "all_ses_ratios = []\n",
    "sort_key_for_ses_analysis = 'loss_dict_daily_cases_RMSE'\n",
    "print(\"Using parameters MAX_MODELS_TO_TAKE_PER_MSA=%i and ACCEPTABLE_LOSS_TOLERANCE=%2.3f\" % \n",
    "      (MAX_MODELS_TO_TAKE_PER_MSA, ACCEPTABLE_LOSS_TOLERANCE))\n",
    "for city in sorted(list(set(non_ablation_df['MSA_name']))):\n",
    "    \n",
    "    city_df = non_ablation_df.loc[(non_ablation_df['MSA_name'] == city)]\n",
    "    min_loss = city_df[sort_key_for_ses_analysis].min()\n",
    "    city_df = (city_df.loc[city_df[sort_key_for_ses_analysis] <= (min_loss * ACCEPTABLE_LOSS_TOLERANCE)]\n",
    "               .sort_values(by=sort_key_for_ses_analysis)\n",
    "               .iloc[:MAX_MODELS_TO_TAKE_PER_MSA])\n",
    "    max_ratio = city_df[sort_key_for_ses_analysis].max() / city_df[sort_key_for_ses_analysis].min()\n",
    "    assert max_ratio > 1 and max_ratio < ACCEPTABLE_LOSS_TOLERANCE\n",
    "    print(\"Plotting %i models for %s\" % (len(city_df), city))\n",
    "    for k in ['p_black', 'median_household_income', 'p_white']:\n",
    "\n",
    "        for comparison in ['medians', 'deciles']:\n",
    "            if comparison == 'medians':\n",
    "                disadvantaged_ratios = get_LIR_ratios_from_models(\n",
    "                        city_df['timestring'], \n",
    "                        f'{k}_below_median',\n",
    "                        f'{k}_above_median')\n",
    "\n",
    "            else:\n",
    "                disadvantaged_ratios = get_LIR_ratios_from_models(\n",
    "                        city_df['timestring'], \n",
    "                        f'{k}_bottom_decile',\n",
    "                        f'{k}_top_decile')\n",
    "                    \n",
    "            if k == 'p_black':\n",
    "                disadvantaged_ratios = 1 / disadvantaged_ratios\n",
    "                \n",
    "            # disadvantaged ratios here is n_models x n_seeds\n",
    "            SES_results.append({\"city\":city, \n",
    "                                'demo':k, \n",
    "                                'comparison':comparison, \n",
    "                                'median_ratio':np.median(disadvantaged_ratios),\n",
    "                                'disadvantaged_group_is_more_sick':np.mean(disadvantaged_ratios > 1), \n",
    "                               'n':len(city_df)})\n",
    "            all_ses_ratios.append(pd.DataFrame({'city':city, \n",
    "                                                'ratio':disadvantaged_ratios.flatten(),\n",
    "                                                'comparison':comparison, \n",
    "                                                'demo':k}))\n",
    "\n",
    "    #SES_results.append({\"city\":city, 'demo':k, 'comparison':'medians', val:above_median_larger})\n",
    "all_ses_ratios = pd.concat(all_ses_ratios)\n",
    "all_ses_ratios['MSA'] = all_ses_ratios['city'].map(lambda x:MSAS_TO_PRETTY_NAMES[x])\n",
    "SES_results = pd.DataFrame(SES_results)\n",
    "SES_results.loc[(SES_results['demo'] == 'median_household_income')].sort_values(by=['city', 'comparison'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plot for paper\n",
    "for comparison in ['deciles', 'medians']:\n",
    "    for demographic in ['p_white', 'median_household_income']:\n",
    "        make_ses_disparities_infection_ratio_plot_for_paper(all_ses_ratios, \n",
    "                                                    comparisons=[comparison], \n",
    "                                                    demographic=demographic, \n",
    "                                                    filename='covid_figures_for_paper/infection_rate_disparities_%s_%s.pdf' \n",
    "                                                            % (demographic, comparison))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3c: disparate impact within POI categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best fit models\n",
    "best_models = []\n",
    "min_timestring = '2020_05_30_14_28_28_220347'\n",
    "max_timestring = '2020_06_16'\n",
    "required_properties = {'experiment_to_run':'rerun_best_models_and_save_cases_per_poi'}\n",
    "for msa in MSAS:\n",
    "    best_models.append(evaluate_all_fitted_models_for_msa(msa, \n",
    "                                                      min_timestring=min_timestring,\n",
    "                                                      max_timestring=max_timestring,\n",
    "                                                      required_properties=required_properties,\n",
    "                                                      key_to_sort_by=None))\n",
    "best_models_df = pd.concat(best_models)\n",
    "best_models_df['MSA_name'] = best_models_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "print(\"Total models loaded: %i\" % len(best_models_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frac_infections_at_each_category_for_groups(timestring, groups, categories_to_plot, poi_categories):\n",
    "    _, _, _, model_results, fast_to_load_results = load_model_and_data_from_timestring(timestring, \n",
    "                                                                                       load_fast_results_only=False,\n",
    "                                                                                       load_full_model=False)\n",
    "    group2fracs = {group:[] for group in groups}\n",
    "    for group in groups:\n",
    "        num_cases_per_poi = model_results['history'][group]['num_cases_per_poi']\n",
    "        n_seeds, n_pois = num_cases_per_poi.shape\n",
    "        assert n_pois == len(poi_categories)\n",
    "        pop_size = model_results['history'][group]['total_pop']\n",
    "        for s in range(n_seeds):\n",
    "            frac_pop_infected = []\n",
    "            for cat in categories_to_plot:\n",
    "                cat_idx = poi_categories == cat \n",
    "                num_cases_at_cat = np.sum(num_cases_per_poi[s][cat_idx])\n",
    "                frac_pop_infected.append(num_cases_at_cat / pop_size)\n",
    "            group2fracs[group].append(frac_pop_infected)\n",
    "    return group2fracs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frac_infected_per_category_for_multiple_models(results_df, msa_name, ax, num_categories=20):\n",
    "    categories_to_plot = PRETTY_TOP_CATEGORIES[:num_categories]\n",
    "    msa_df = results_df[results_df['MSA_name'] == msa_name]\n",
    "    poi_categories = MSA_TO_STUFF[msa_name]['poi_categories']\n",
    "    poi_categories['pretty_sub_category'] = poi_categories['sub_category'].map(lambda x:SUBCATEGORIES_TO_PRETTY_NAMES[x] if x in SUBCATEGORIES_TO_PRETTY_NAMES else x)\n",
    "    bottom_decile_fracs = []  # num_models x num_categories\n",
    "    top_decile_fracs = []  # num_models x num_categories\n",
    "    for ts in msa_df.timestring.values:\n",
    "        group2fracs = get_frac_infections_at_each_category_for_groups(ts, [LOWINCOME, HIGHINCOME], categories_to_plot, poi_categories.pretty_sub_category.values)\n",
    "        bottom_decile_fracs.extend(group2fracs[LOWINCOME])\n",
    "        top_decile_fracs.extend(group2fracs[HIGHINCOME])\n",
    "    \n",
    "    print('Num params * seeds:', len(bottom_decile_fracs))\n",
    "    bottom_decile_fracs = np.array(bottom_decile_fracs)\n",
    "    bottom_decile_mean = INCIDENCE_POP * np.mean(bottom_decile_fracs, axis=0)\n",
    "    bottom_decile_min = INCIDENCE_POP * np.percentile(bottom_decile_fracs, LOWER_PERCENTILE, axis=0)\n",
    "    bottom_decile_max = INCIDENCE_POP * np.percentile(bottom_decile_fracs, UPPER_PERCENTILE, axis=0)    \n",
    "    top_decile_fracs = np.array(top_decile_fracs)\n",
    "    top_decile_mean = INCIDENCE_POP * np.mean(top_decile_fracs, axis=0)\n",
    "    top_decile_min = INCIDENCE_POP * np.percentile(top_decile_fracs, LOWER_PERCENTILE, axis=0)\n",
    "    top_decile_max = INCIDENCE_POP * np.percentile(top_decile_fracs, UPPER_PERCENTILE, axis=0)\n",
    "\n",
    "    y_pos = range(num_categories)\n",
    "    sorted_idx = np.argsort(bottom_decile_mean)  # sort category plotting by their impact\n",
    "    ax.plot(bottom_decile_mean[sorted_idx], y_pos, label='bottom income decile', color='darkorchid', linewidth=2)\n",
    "    ax.fill_betweenx(y=y_pos, x1=bottom_decile_min[sorted_idx],\n",
    "                     x2=bottom_decile_max[sorted_idx], color='darkorchid', alpha=.3)\n",
    "    ax.plot(top_decile_mean[sorted_idx], y_pos, label='top income decile', color='darkgoldenrod', linewidth=2)\n",
    "    ax.fill_betweenx(y=y_pos, x1=top_decile_min[sorted_idx],\n",
    "                     x2=top_decile_max[sorted_idx], color='darkgoldenrod', alpha=.5)\n",
    "    ax.set_yticks(y_pos)\n",
    "    labels = [categories_to_plot[i] for i in sorted_idx]\n",
    "    ax.set_yticklabels(labels, fontsize=16)\n",
    "    ax.tick_params(axis='x', labelsize=16)\n",
    "    ax.set_xlabel('Cumulative infections (per 100k) at category', fontsize=16)\n",
    "    curr_lim = ax.get_xlim()\n",
    "    ax.set_xlim(0, curr_lim[1])\n",
    "    ax.grid(alpha=.3)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "category_order = plot_frac_infected_per_category_for_multiple_models(best_models_df, DC_NAME, ax)\n",
    "ax.legend(fontsize=16, loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3d: mobility over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mobility_comparison_line_plot(msa_name, min_datetime, max_datetime, \n",
    "                                       group1, group1_label, group1_color, \n",
    "                                       group2, group2_label, group2_color, \n",
    "                                       ax, num_hours_to_agg=24, set_labels=True):\n",
    "    stuff = MSA_TO_STUFF[msa_name]\n",
    "    cbg_idx_groups_to_track, cbg_sizes = stuff['cbg_idx_groups_to_track'], stuff['cbg_sizes']\n",
    "    poi_cbg_visits_list = stuff['poi_cbg_visits_list']    \n",
    "    hours = helper.list_hours_in_range(min_datetime, max_datetime)\n",
    "    assert (len(hours) % num_hours_to_agg) == 0\n",
    "    hours_to_plot = [hours[t] for t in np.arange(0, len(hours), num_hours_to_agg)]\n",
    "    num_pois, num_cbgs = poi_cbg_visits_list[0].shape\n",
    "    indicator_1 = np.zeros(num_cbgs)\n",
    "    indicator_1[cbg_idx_groups_to_track[group1]] = 1.0\n",
    "    pop_size_1 = np.sum(cbg_sizes[cbg_idx_groups_to_track[group1]])\n",
    "    indicator_2 = np.zeros(num_cbgs)\n",
    "    indicator_2[cbg_idx_groups_to_track[group2]] = 1.0\n",
    "    pop_size_2 = np.sum(cbg_sizes[cbg_idx_groups_to_track[group2]])\n",
    "    Y1 = []\n",
    "    Y2 = []\n",
    "    for t in range(len(hours_to_plot)):\n",
    "        total_1 = 0\n",
    "        total_2 = 0\n",
    "        start_hr = t*num_hours_to_agg\n",
    "        for hr in range(start_hr, start_hr+num_hours_to_agg):\n",
    "            total_1 += np.sum(poi_cbg_visits_list[hr] @ indicator_1)\n",
    "            total_2 += np.sum(poi_cbg_visits_list[hr] @ indicator_2)\n",
    "        Y1.append(total_1 / pop_size_1)  # per-capita num visits over time period\n",
    "        Y2.append(total_2 / pop_size_2)\n",
    "    ax.plot_date(hours_to_plot, Y1, linestyle='-', marker='.', linewidth=2, label=group1_label, color=group1_color)\n",
    "    ax.plot_date(hours_to_plot, Y2, linestyle='-', marker='.', linewidth=2, label=group2_label, color=group2_color)\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(mdates.SU, interval=2))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.grid(alpha=0.2)\n",
    "    \n",
    "    if set_labels:\n",
    "        ax.set_xlabel('Date', fontsize=16)\n",
    "        ax.set_ylabel('Per capita mobility', fontsize=16)\n",
    "        ax.set_title(MSAS_TO_PRETTY_NAMES[msa_name], fontsize=18)\n",
    "        ax.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "make_mobility_comparison_line_plot(DC_NAME, MIN_DATETIME, MAX_DATETIME, \n",
    "                                   LOWINCOME, 'bottom income decile', 'darkorchid',\n",
    "                                   HIGHINCOME, 'top income decile', 'darkgoldenrod', ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "axes = [ax for axes_row in axes for ax in axes_row]\n",
    "for i, (ax, msa_name) in enumerate(list(zip(axes, MSAS))):\n",
    "    print(msa_name)\n",
    "    ax.set_title(MSAS_TO_PRETTY_NAMES[msa_name], fontsize=14)\n",
    "    make_mobility_comparison_line_plot(msa_name, MIN_DATETIME, MAX_DATETIME, \n",
    "                                       LOWINCOME, 'bottom income decile', 'darkorchid',\n",
    "                                       HIGHINCOME, 'top income decile', 'darkgoldenrod', \n",
    "                                       ax, set_labels=False)\n",
    "    ax.set_title(MSAS_TO_PRETTY_NAMES[msa_name], fontsize=18)\n",
    "    if i == 0:\n",
    "        ax.legend(bbox_to_anchor=(-0.5, 2), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "axes = [ax for axes_row in axes for ax in axes_row]\n",
    "for i, (ax, msa_name) in enumerate(list(zip(axes, MSAS))):\n",
    "    print(msa_name)\n",
    "    ax.set_title(MSAS_TO_PRETTY_NAMES[msa_name], fontsize=14)\n",
    "    make_mobility_comparison_line_plot(msa_name, MIN_DATETIME, MAX_DATETIME, \n",
    "                                       NONWHITE, 'bottom decile (% white)', 'forestgreen',\n",
    "                                       WHITE, 'top decile (% white)', 'salmon', \n",
    "                                       ax, set_labels=False)\n",
    "    ax.set_title(MSAS_TO_PRETTY_NAMES[msa_name], fontsize=18)\n",
    "    if i == 0:\n",
    "        ax.legend(bbox_to_anchor=(-0.5, 2), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 3e: transmission rates per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_category_comparison_scatter_plot(attributes_df_1, attributes_df_2,\n",
    "                                          attribute_to_plot, ax, color, title,\n",
    "                                          xlabel, ylabel, categories_to_label, \n",
    "                                          x_lim=None, y_lim=None, plot_log=False,\n",
    "                                          psi=1):\n",
    "\n",
    "    categories = attributes_df_1.category.values\n",
    "    vals_1 = attributes_df_1[attribute_to_plot].values\n",
    "    visits_1 = attributes_df_1['num_visits_per_capita'].values\n",
    "    vals_2 = attributes_df_2[attribute_to_plot].values\n",
    "    visits_2 = attributes_df_2['num_visits_per_capita'].values\n",
    "    avg_visits = (visits_1 + visits_2) / 2\n",
    "\n",
    "    X = vals_2\n",
    "    Y = vals_1\n",
    "    if 'transmission_rate' in attribute_to_plot:\n",
    "        X = X * psi\n",
    "        Y = Y * psi\n",
    "    sizes = 200 * avg_visits\n",
    "    if plot_log:\n",
    "        X = np.log(X)\n",
    "        Y = np.log(Y)\n",
    "        xlabel += ' (log)'\n",
    "        ylabel += ' (log)'\n",
    "    ax.scatter(X, Y, s=sizes, alpha=0.6, color=color)\n",
    "    \n",
    "    max_val = max(max(X), max(Y))  # set x and y axes to have the same tick ranges\n",
    "    offset = .1 * max_val\n",
    "    if x_lim is None:\n",
    "        ax.set_xlim(0 - offset, max_val + offset)\n",
    "    else:\n",
    "        ax.set_xlim(x_lim[0], x_lim[1])\n",
    "    if y_lim is None:\n",
    "        ax.set_ylim(0 - offset, max_val + offset)\n",
    "    else:\n",
    "        ax.set_ylim(y_lim[0], y_lim[1])\n",
    "    ax.plot(list(ax.get_xlim()), list(ax.get_xlim()), color='grey', alpha=0.5, label='y=x')\n",
    "    \n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.grid(alpha=.2)\n",
    "    ax.legend(fontsize=16)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_xlabel(xlabel, fontsize=16)\n",
    "    ax.set_ylabel(ylabel, fontsize=16)  \n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        cat = categories[i]\n",
    "        if cat in categories_to_label:\n",
    "            if attribute_to_plot == 'avg_transmission_rate' and 'Washington' in title:\n",
    "            # special code to format Washington DC plot\n",
    "                if cat in {'Fitness Centers', 'Offices of Physicians', 'Hotels & Motels'}:\n",
    "                    ax.annotate(cat, xy=(X[i], Y[i]), xytext=(-100, 0), textcoords='offset pixels', fontsize=14)\n",
    "                elif cat in {'Full-Service Restaurants', 'Religious Organizations'}:\n",
    "                    ax.annotate(cat, xy=(X[i], Y[i]), xytext=(-150, 0), textcoords='offset pixels', fontsize=14)\n",
    "                elif cat in {'Convenience Stores', 'Department Stores', 'Other General Stores', \n",
    "                             'Used Merchandise Stores'}:\n",
    "                    ax.annotate(cat, xy=(X[i], Y[i]), xytext=(-10, -5), textcoords='offset pixels', fontsize=14)\n",
    "                elif cat in {'Gas Stations', 'Grocery Stores'}:\n",
    "                    ax.annotate(cat, xy=(X[i], Y[i]), xytext=(-50, 5), textcoords='offset pixels', fontsize=14)\n",
    "                else:\n",
    "                    ax.annotate(cat, xy=(X[i], Y[i]), xytext=(-10, 0), textcoords='offset pixels', fontsize=14)\n",
    "            else:\n",
    "                ax.annotate(cat, xy=(X[i], Y[i]), xytext=(-10, 0), textcoords='offset pixels', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transmission_rate_scatter_plot_for_paper(save_fig=False):\n",
    "    categories_to_plot = PRETTY_TOP_CATEGORIES[:12]\n",
    "    categories_to_label = categories_to_plot[:12]\n",
    "    stuff = MSA_TO_STUFF[DC_NAME]\n",
    "    cbg_idx_groups_to_track = stuff['cbg_idx_groups_to_track']\n",
    "    cbg_sizes = stuff['cbg_sizes']\n",
    "\n",
    "    li_poi_attr = get_poi_attributes_for_msa(DC_NAME, group_to_track=LOWINCOME)\n",
    "    li_pop_size = np.sum(cbg_sizes[cbg_idx_groups_to_track[LOWINCOME]])\n",
    "    li_cat_attr = get_category_attributes_from_poi_attributes(li_poi_attr, categories_to_plot, pop_size=li_pop_size)\n",
    "    hi_poi_attr = get_poi_attributes_for_msa(DC_NAME, group_to_track=HIGHINCOME)\n",
    "    hi_pop_size = np.sum(cbg_sizes[cbg_idx_groups_to_track[HIGHINCOME]])\n",
    "    hi_cat_attr = get_category_attributes_from_poi_attributes(hi_poi_attr, categories_to_plot, pop_size=hi_pop_size)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    attribute = 'avg_transmission_rate'\n",
    "    title = '%s: comparison of transmission rates' % MSAS_TO_PRETTY_NAMES[DC_NAME]\n",
    "    xlabel = 'Avg transmission rate,\\ntop income decile'\n",
    "    ylabel = 'Avg transmission rate,\\nbottom income decile'\n",
    "    make_category_comparison_scatter_plot(li_cat_attr, hi_cat_attr, \n",
    "                                          attribute, ax, 'coral', title,\n",
    "                                          xlabel, ylabel, categories_to_label, psi=4500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_transmission_rate_scatter_plot_for_paper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplement: transmission rate ratio tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_ratios_for_all_msas(group1, group2, num_categories=20):\n",
    "    categories_to_plot = PRETTY_TOP_CATEGORIES[:num_categories]\n",
    "    attributes = ['avg_area', 'avg_dwell_time_correction_factor', 'avg_occupancy', 'avg_density', 'avg_transmission_rate']\n",
    "    col_names = ['%s_ratio' % a for a in attributes]\n",
    "    all_results = []\n",
    "    for msa_name in MSAS:\n",
    "        print('Getting results for', msa_name)\n",
    "        poi_attr_1 = get_poi_attributes_for_msa(msa_name, group_to_track=group1)\n",
    "        cat_attr_1 = get_category_attributes_from_poi_attributes(poi_attr_1, categories_to_plot)\n",
    "        poi_attr_2 = get_poi_attributes_for_msa(msa_name, group_to_track=group2)\n",
    "        cat_attr_2 = get_category_attributes_from_poi_attributes(poi_attr_2, categories_to_plot)\n",
    "        df = pd.DataFrame(cat_attr_1[attributes].values / cat_attr_2[attributes].values, columns=col_names)\n",
    "        df['category'] = categories_to_plot\n",
    "        df['MSA_name'] = msa_name\n",
    "        all_results.append(df)\n",
    "    all_results = pd.concat(all_results)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ses_results = get_attribute_ratios_for_all_msas(LOWINCOME, HIGHINCOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make summary dataframe of avg_transmission_rate ratios for MSA x category\n",
    "categories_to_plot = PRETTY_TOP_CATEGORIES[:20]\n",
    "mat = np.zeros((len(categories_to_plot), len(MSAS)))\n",
    "for i, cat in enumerate(categories_to_plot):\n",
    "    subdf = all_ses_results[all_ses_results['category'] == cat]\n",
    "    for j, msa_name in enumerate(MSAS):\n",
    "        results = subdf[subdf['MSA_name'] == msa_name]\n",
    "        assert len(results) == 1\n",
    "        mat[i, j] = round(results['avg_transmission_rate_ratio'], 3)\n",
    "\n",
    "# median ratio per msa, aggregated over categories\n",
    "msa_medians = np.round(np.median(mat, axis=0), 3)\n",
    "for msa, median in zip(MSAS, msa_medians):\n",
    "    print(msa, median)\n",
    "\n",
    "cols = [MSAS_TO_PRETTY_NAMES[m] for m in MSAS]\n",
    "summary_df = pd.DataFrame(mat, columns=cols, index=categories_to_plot)\n",
    "summary_df['Med'] = np.round(np.median(summary_df.values, axis=1), 3)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_race_results = get_attribute_ratios_for_all_msas(NONWHITE, WHITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create category x MSA df of ratios of avg transmission rates\n",
    "categories_to_plot = PRETTY_TOP_CATEGORIES[:20]\n",
    "msas = all_race_results.MSA_name.unique()\n",
    "mat = np.zeros((len(categories_to_plot), len(msas)))\n",
    "for i, cat in enumerate(categories_to_plot):\n",
    "    subdf = all_race_results[all_race_results['category'] == cat]\n",
    "    for j, msa_name in enumerate(msas):\n",
    "        results = subdf[subdf['MSA_name'] == msa_name]\n",
    "        assert len(results) == 1\n",
    "        mat[i, j] = round(results['avg_transmission_rate_ratio'], 3)\n",
    "\n",
    "# median ratio per msa, aggregated over categories\n",
    "msa_medians = np.round(np.median(mat, axis=0), 3)\n",
    "for msa, median in zip(MSAS, msa_medians):\n",
    "    print(msa, median)\n",
    "\n",
    "cols = [MSAS_TO_PRETTY_NAMES[m] for m in msas]\n",
    "summary_df = pd.DataFrame(mat, columns=cols, index=categories_to_plot)\n",
    "summary_df['Med'] = np.round(np.median(summary_df.values, axis=1), 3)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplement: num visits per capita to POI categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_capita_category_visits(msa_name, group1, group1_label, group1_color, \n",
    "                                    group2, group2_label, group2_color, \n",
    "                                    ax, num_categories=20):\n",
    "    categories_to_plot = PRETTY_TOP_CATEGORIES[:num_categories]\n",
    "    cbg_sizes = MSA_TO_STUFF[msa_name]['cbg_sizes']\n",
    "    cbg_idx_groups_to_track = MSA_TO_STUFF[msa_name]['cbg_idx_groups_to_track']\n",
    "    poi_attr_1 = get_poi_attributes_for_msa(msa_name, group_to_track=group1)\n",
    "    group1_pop_size = np.sum(cbg_sizes[cbg_idx_groups_to_track[group1]])\n",
    "    cat_attr_1 = get_category_attributes_from_poi_attributes(poi_attr_1, categories_to_plot, pop_size=group1_pop_size)\n",
    "    X1 = cat_attr_1.num_visits_per_capita.values\n",
    "    \n",
    "    poi_attr_2 = get_poi_attributes_for_msa(msa_name, group_to_track=group2)\n",
    "    group2_pop_size = np.sum(cbg_sizes[cbg_idx_groups_to_track[group2]])\n",
    "    cat_attr_2 = get_category_attributes_from_poi_attributes(poi_attr_2, categories_to_plot, pop_size=group2_pop_size)\n",
    "    X2 = cat_attr_2.num_visits_per_capita.values\n",
    "    \n",
    "    bar_pos = np.arange(0, 3*num_categories, 3)\n",
    "    ax.barh(bar_pos-1, X1, align='center', label=group1_label, color=group1_color)\n",
    "    ax.barh(bar_pos, X2, align='center', label=group2_label, color=group2_color)\n",
    "    ax.set_yticks(bar_pos-.5)\n",
    "    ax.set_yticklabels(categories_to_plot, fontsize=16)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.tick_params(axis='x', labelsize=16)        \n",
    "    ax.set_xlabel('Per capita visits to category', fontsize=16)\n",
    "    ax.legend(fontsize=16, loc='lower right')\n",
    "    ax.set_title(MSAS_TO_PRETTY_NAMES[msa_name], fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "plot_per_capita_category_visits(DC_NAME, LOWINCOME, 'bottom income decile', 'darkorchid',\n",
    "                                HIGHINCOME, 'top income decile', 'darkgoldenrod', ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safegraph",
   "language": "python",
   "name": "safegraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
