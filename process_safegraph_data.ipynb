{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from covid_constants_and_util import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import json\n",
    "import datetime\n",
    "import copy\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "import helper_methods_for_aggregate_data_analysis as helper\n",
    "import h5py\n",
    "import re\n",
    "\n",
    "JUST_TESTING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we don't append onto existing files. \n",
    "assert not os.path.exists(os.path.join(helper.ANNOTATED_H5_DATA_DIR, helper.CHUNK_FILENAME))\n",
    "assert not os.path.exists(os.path.join(helper.H5_DATA_DIR, helper.CHUNK_FILENAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in individual dataframes for monthly and weekly data [raw SafeGraph data].\n",
    "\n",
    "dask.config.set(pool=ThreadPool(20))\n",
    "\n",
    "all_monthly_dfs = []\n",
    "all_weekly_dfs = []   \n",
    "\n",
    "for week_string in helper.ALL_WEEKLY_STRINGS:\n",
    "    all_weekly_dfs.append(helper.load_patterns_data(week_string=week_string, just_testing=JUST_TESTING))\n",
    "    \n",
    "for month, year in [\n",
    "             (1, 2019),(2, 2019),(3, 2019),(4, 2019),(5, 2019),(6, 2019),(7, 2019),(8, 2019),(9, 2019),(10, 2019),(11, 2019),(12, 2019),\n",
    "             (1, 2020),(2, 2020)][::-1]:\n",
    "    # Note ::-1: we load most recent files first so we will take their places info if it is available.\n",
    "    all_monthly_dfs.append(helper.load_patterns_data(month=month, year=year, just_testing=JUST_TESTING))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge monthly DFs into a single dataframe. Each row is one POI. \n",
    "\n",
    "base = all_monthly_dfs[0]\n",
    "core = all_monthly_dfs[1].columns.intersection(base.columns).to_list()\n",
    "for i, df in enumerate(all_monthly_dfs[1:]):\n",
    "    print(i)\n",
    "    # merge all new places into base so that core info is not nan for new sgids\n",
    "    new_places = df.loc[df.index.difference(base.index)][core]\n",
    "    base = pd.concat([base, new_places], join='outer', sort=False)\n",
    "    # can now left merge in the df because all sgids will be in base\n",
    "    cols_to_use = df.columns.difference(base.columns).to_list()\n",
    "    base =  pd.merge(base, df[cols_to_use], left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in weekly dataframes. Just merge on SafeGraph ID, left merge. \n",
    "# This means that our final POI set is those that have both monthly and weekly data. \n",
    "# at the end of this cell we will have a single dataframe. \n",
    "\n",
    "for i, weekly_df in enumerate(all_weekly_dfs):\n",
    "    print(\"\\n\\n********Weekly dataframe %i/%i\" % (i + 1, len(all_weekly_dfs)))\n",
    "    assert len(base.columns.intersection(weekly_df.columns)) == 0\n",
    "    \n",
    "    ids_in_weekly_but_not_monthly = set(weekly_df.index) - set(base.index)\n",
    "    print(\"Warning: %i/%i POIs in weekly but not monthly data; dropping these\" % (len(ids_in_weekly_but_not_monthly), \n",
    "                                                                  len(df)))\n",
    "    base = pd.merge(base, weekly_df, how='left', left_index=True, right_index=True, validate='one_to_one')\n",
    "    print(\"Missing data for weekly columns\")\n",
    "    print(pd.isnull(base[weekly_df.columns]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# annotate with demographic info and save dataframe. Dataframe is saved in h5py format, separated into chunks. \n",
    "\n",
    "test = base.sample(frac=1) # shuffle so rows are in random order [in case we want to prototype on subset].\n",
    "helper.annotate_with_demographic_info_and_write_out_in_chunks(test, just_testing=JUST_TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload because we don't modify DF in place. \n",
    "test = helper.load_all_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by MSA and write out outfiles.  \n",
    "just_in_msas = test.loc[test['poi_lat_lon_Metropolitan/Micropolitan Statistical Area'] == 'Metropolitan Statistical Area']\n",
    "assert pd.isnull(just_in_msas['poi_lat_lon_CBSA Title']).sum() == 0\n",
    "print(\"%i/%i POIs are in MSAs (%i MSAs total)\" % (len(just_in_msas), \n",
    "                                                  len(test), \n",
    "                                                  len(set(just_in_msas['poi_lat_lon_CBSA Title']))))\n",
    "grouped_by_msa = just_in_msas.groupby('poi_lat_lon_CBSA Title')\n",
    "total_written_out = 0\n",
    "for msa_name, small_d in grouped_by_msa:\n",
    "    small_d = small_d.copy().sample(frac=1) # make sure rows in random order. \n",
    "    small_d.index = range(len(small_d))\n",
    "    name_without_spaces = re.sub('[^0-9a-zA-Z]+', '_', msa_name)\n",
    "    filename = os.path.join(helper.STRATIFIED_BY_AREA_DIR, '%s.csv' % name_without_spaces)\n",
    "    for k in ['aggregated_cbg_population_adjusted_visitor_home_cbgs', 'aggregated_visitor_home_cbgs']:\n",
    "        small_d[k] = small_d[k].map(lambda x:json.dumps(dict(x))) # cast to json so properly saved in CSV. \n",
    "    small_d.to_csv(filename)\n",
    "    print(\"Wrote out dataframe with %i POIs to %s\" % (len(small_d), '%s.csv' % name_without_spaces))\n",
    "    total_written_out += 1\n",
    "print(\"Total written out: %i\" % total_written_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safegraph",
   "language": "python",
   "name": "safegraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
